{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "from shapely import wkt\n",
    "from folium.features import GeoJsonTooltip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POPULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.features import GeoJsonTooltip\n",
    "from shapely import wkt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Read and prepare lines data\n",
    "# ---------------------------\n",
    "raw_df = pd.read_csv('data\\\\dev_wings_agg_span_2024_01_01.csv')\n",
    "df_lines = raw_df[['shape', 'cust_industrial', 'cust_commercial', 'cust_residential', 'cust_sensitive',\n",
    "                     'cust_essential', 'cust_medicalcert', 'cust_urgent', 'cust_lifesupport', 'cust_total',\n",
    "                     'downstream_cust_industrial', 'downstream_cust_commercial', 'downstream_cust_residential',\n",
    "                     'downstream_cust_sensitive', 'downstream_cust_essential', 'downstream_cust_medicalcert',\n",
    "                     'downstream_cust_urgent', 'downstream_cust_lifesupport', 'downstream_cust_total']]\n",
    "\n",
    "# Convert the 'shape' column (WKT strings) to geometry objects\n",
    "df_lines[\"geometry\"] = df_lines[\"shape\"].apply(wkt.loads)\n",
    "\n",
    "gdf_lines = gpd.GeoDataFrame(df_lines, geometry=\"geometry\", crs=\"EPSG:2230\")\n",
    "\n",
    "gdf_lines = gdf_lines.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Read and prepare polygon data\n",
    "# ---------------------------\n",
    "subdiv_url = \"https://www2.census.gov/geo/tiger/TIGER2020/COUSUB/tl_2020_06_cousub.zip\"\n",
    "gdf_subdiv = gpd.read_file(subdiv_url)\n",
    "gdf_sd = gdf_subdiv[gdf_subdiv['COUNTYFP'] == '073']\n",
    "\n",
    "places_url = \"https://www2.census.gov/geo/tiger/TIGER2020/PLACE/tl_2020_06_place.zip\"\n",
    "gdf_places = gpd.read_file(places_url)\n",
    "counties_url = \"https://www2.census.gov/geo/tiger/TIGER2020/COUNTY/tl_2020_us_county.zip\"\n",
    "gdf_counties = gpd.read_file(counties_url)\n",
    "gdf_sd_county = gdf_counties[gdf_counties['NAME'] == 'San Diego']\n",
    "\n",
    "# Project places and counties to EPSG:4326\n",
    "gdf_places = gdf_places.to_crs(\"EPSG:4326\")\n",
    "gdf_sd_county = gdf_sd_county.to_crs(\"EPSG:4326\")\n",
    "\n",
    "gdf_sd_places = gpd.sjoin(gdf_places, gdf_sd_county, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "# Remove any pre-existing 'index_right' column to avoid conflicts\n",
    "if 'index_right' in gdf_sd_places.columns:\n",
    "    gdf_sd_places = gdf_sd_places.drop(columns=['index_right'])\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Spatial join: Accumulate customer values for lines within polygons\n",
    "# ---------------------------\n",
    "lines_in_polygons_within = gpd.sjoin(gdf_lines, gdf_sd_places, how=\"inner\", predicate=\"within\")\n",
    "print(\"Number of matched lines with 'within':\", len(lines_in_polygons_within))\n",
    "\n",
    "if len(lines_in_polygons_within) == 0:\n",
    "    lines_in_polygons = gpd.sjoin(gdf_lines, gdf_sd_places, how=\"inner\", predicate=\"intersects\")\n",
    "    print(\"Number of matched lines with 'intersects':\", len(lines_in_polygons))\n",
    "else:\n",
    "    lines_in_polygons = lines_in_polygons_within\n",
    "\n",
    "customer_fields = [\n",
    "    'cust_industrial', 'cust_commercial', 'cust_residential', 'cust_sensitive',\n",
    "    'cust_essential', 'cust_medicalcert', 'cust_urgent', 'cust_lifesupport', 'cust_total'\n",
    "]\n",
    "\n",
    "accumulated = lines_in_polygons.groupby(\"index_right\")[customer_fields].sum()\n",
    "\n",
    "gdf_sd_places = gdf_sd_places.merge(accumulated, left_index=True, right_index=True, how='left')\n",
    "\n",
    "gdf_sd_places[customer_fields] = gdf_sd_places[customer_fields].fillna(0)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Create a Folium map\n",
    "# ---------------------------\n",
    "center = [gdf_sd_places.geometry.centroid.y.mean(), gdf_sd_places.geometry.centroid.x.mean()]\n",
    "m = folium.Map(location=center, zoom_start=10)\n",
    "\n",
    "tooltip = GeoJsonTooltip(\n",
    "    fields=customer_fields,\n",
    "    aliases=[field.replace(\"cust_\", \"\").capitalize() for field in customer_fields],\n",
    "    localize=True,\n",
    "    sticky=False,\n",
    "    labels=True,\n",
    "    style=\"\"\"\n",
    "        background-color: #F0EFEF;\n",
    "        border: 1px solid black;\n",
    "        border-radius: 3px;\n",
    "        box-shadow: 3px;\n",
    "    \"\"\",\n",
    "    max_width=800,\n",
    ")\n",
    "\n",
    "folium.GeoJson(\n",
    "    gdf_sd_places,\n",
    "    tooltip=tooltip,\n",
    ").add_to(m)\n",
    "\n",
    "# Uncomment to save the folium map to an HTML file\n",
    "# m.save(\"map.html\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Save the accumulated polygon data to CSV\n",
    "# ---------------------------\n",
    "gdf_sd_places[\"geometry\"] = gdf_sd_places[\"geometry\"].apply(lambda x: x.wkt)\n",
    "gdf_sd_places.to_csv(\"accumulated_polygon_values.csv\", index=False)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from shapely import wkt\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load and prepare lines data\n",
    "# ---------------------------\n",
    "raw_df = pd.read_csv('data\\\\dev_wings_agg_span_2024_01_01.csv')\n",
    "df_lines = raw_df[['shape', 'cust_industrial', 'cust_commercial', 'cust_residential', \n",
    "                     'cust_sensitive', 'cust_essential', 'cust_medicalcert', 'cust_urgent', \n",
    "                     'cust_lifesupport', 'cust_total', 'downstream_cust_industrial', \n",
    "                     'downstream_cust_commercial', 'downstream_cust_residential', \n",
    "                     'downstream_cust_sensitive', 'downstream_cust_essential', \n",
    "                     'downstream_cust_medicalcert', 'downstream_cust_urgent', \n",
    "                     'downstream_cust_lifesupport', 'downstream_cust_total']]\n",
    "\n",
    "df_lines[\"geometry\"] = df_lines[\"shape\"].apply(wkt.loads)\n",
    "\n",
    "initial_crs = \"EPSG:2230\"\n",
    "gdf_lines = gpd.GeoDataFrame(df_lines, geometry=\"geometry\", crs=initial_crs)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Check bounds before and after reprojection\n",
    "# ---------------------------\n",
    "print(\"Original bounds (in {}):\".format(initial_crs), gdf_lines.total_bounds)\n",
    "gdf_lines = gdf_lines.to_crs(\"EPSG:4326\")\n",
    "print(\"Reprojected bounds (EPSG:4326):\", gdf_lines.total_bounds)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Sample 50,000 random line\n",
    "# ---------------------------\n",
    "if len(gdf_lines) > 50000:\n",
    "    sample_gdf = gdf_lines.sample(n=50000, random_state=42)\n",
    "else:\n",
    "    sample_gdf = gdf_lines.copy()\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Create a Folium map with the sampled lines\n",
    "# ---------------------------\n",
    "center = [sample_gdf.geometry.centroid.y.mean(), sample_gdf.geometry.centroid.x.mean()]\n",
    "m = folium.Map(location=center, zoom_start=10)\n",
    "\n",
    "folium.GeoJson(sample_gdf.__geo_interface__, name=\"Random Lines\").add_to(m)\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m.save(\"sampled_lines_map.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.features import GeoJsonTooltip\n",
    "from shapely import wkt\n",
    "import matplotlib.pyplot as plt\n",
    "import branca.colormap as cm\n",
    "\n",
    "def risk_style(feature):\n",
    "    rank = feature['properties']['risk_rank']\n",
    "    return {\n",
    "        'fillColor': colormap(rank),\n",
    "        'color': 'black',\n",
    "        'weight': 1,\n",
    "        'fillOpacity': 0.6,\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load and prepare lines data\n",
    "# ---------------------------\n",
    "raw_df = pd.read_csv('data\\\\dev_wings_agg_span_2024_01_01.csv')\n",
    "df_lines = raw_df[['shape', 'cust_industrial', 'cust_commercial', 'cust_residential', 'cust_sensitive',\n",
    "                     'cust_essential', 'cust_medicalcert', 'cust_urgent', 'cust_lifesupport', 'cust_total',\n",
    "                     'downstream_cust_industrial', 'downstream_cust_commercial', 'downstream_cust_residential',\n",
    "                     'downstream_cust_sensitive', 'downstream_cust_essential', 'downstream_cust_medicalcert',\n",
    "                     'downstream_cust_urgent', 'downstream_cust_lifesupport', 'downstream_cust_total']]\n",
    "\n",
    "df_lines[\"geometry\"] = df_lines[\"shape\"].apply(wkt.loads)\n",
    "\n",
    "gdf_lines = gpd.GeoDataFrame(df_lines, geometry=\"geometry\", crs=\"EPSG:2230\")\n",
    "gdf_lines = gdf_lines.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load and prepare polygon data\n",
    "# ---------------------------\n",
    "subdiv_url = \"https://www2.census.gov/geo/tiger/TIGER2020/COUSUB/tl_2020_06_cousub.zip\"\n",
    "gdf_subdiv = gpd.read_file(subdiv_url)\n",
    "gdf_sd = gdf_subdiv[gdf_subdiv['COUNTYFP'] == '073']\n",
    "gdf_sd = gdf_sd.to_crs(\"EPSG:4326\")\n",
    "\n",
    "places_url = \"https://www2.census.gov/geo/tiger/TIGER2020/PLACE/tl_2020_06_place.zip\"\n",
    "gdf_places = gpd.read_file(places_url)\n",
    "counties_url = \"https://www2.census.gov/geo/tiger/TIGER2020/COUNTY/tl_2020_us_county.zip\"\n",
    "gdf_counties = gpd.read_file(counties_url)\n",
    "gdf_sd_county = gdf_counties[gdf_counties['NAME'] == 'San Diego']\n",
    "\n",
    "gdf_places = gdf_places.to_crs(\"EPSG:4326\")\n",
    "gdf_sd_county = gdf_sd_county.to_crs(\"EPSG:4326\")\n",
    "\n",
    "gdf_sd_places = gpd.sjoin(gdf_places, gdf_sd_county, how=\"inner\", predicate=\"intersects\")\n",
    "if 'index_right' in gdf_sd_places.columns:\n",
    "    gdf_sd_places = gdf_sd_places.drop(columns=['index_right'])\n",
    "gdf_sd_places = gdf_sd_places.loc[:, ~gdf_sd_places.columns.duplicated()]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Create non-overlapping background polygons\n",
    "# ---------------------------\n",
    "places_union = gdf_sd_places.unary_union\n",
    "\n",
    "def subtract_places(geom, union_geom):\n",
    "    return geom.difference(union_geom)\n",
    "\n",
    "gdf_bg_nonoverlap = gdf_sd.copy()\n",
    "gdf_bg_nonoverlap[\"geometry\"] = gdf_bg_nonoverlap.geometry.apply(lambda g: subtract_places(g, places_union))\n",
    "gdf_bg_nonoverlap = gdf_bg_nonoverlap[~gdf_bg_nonoverlap.is_empty]\n",
    "gdf_bg_nonoverlap[\"layer\"] = \"Background\"\n",
    "gdf_sd_places[\"layer\"] = \"Incorporated Places\"\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Accumulate customer values into the non-overlapping polygons\n",
    "# ---------------------------\n",
    "gdf_combined = pd.concat([gdf_bg_nonoverlap, gdf_sd_places], ignore_index=True)\n",
    "\n",
    "lines_in_polygons = gpd.sjoin(gdf_lines, gdf_combined, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "customer_fields = [\n",
    "    'cust_industrial', 'cust_commercial', 'cust_residential', 'cust_sensitive',\n",
    "    'cust_essential', 'cust_medicalcert', 'cust_urgent', 'cust_lifesupport', 'cust_total', \n",
    "    'downstream_cust_industrial', 'downstream_cust_commercial', 'downstream_cust_residential',\n",
    "    'downstream_cust_sensitive', 'downstream_cust_essential', 'downstream_cust_medicalcert',\n",
    "    'downstream_cust_urgent', 'downstream_cust_lifesupport', 'downstream_cust_total'\n",
    "]\n",
    "\n",
    "accumulated = lines_in_polygons.groupby(\"index_right\")[customer_fields].sum()\n",
    "gdf_combined = gdf_combined.merge(accumulated, left_index=True, right_index=True, how='left')\n",
    "gdf_combined[customer_fields] = gdf_combined[customer_fields].fillna(0)\n",
    "\n",
    "# ---- Compute PSPS Risk Score and Rank ----\n",
    "weights = {\n",
    "    'cust_lifesupport': 10,\n",
    "    'cust_medicalcert': 8,\n",
    "    'cust_urgent': 6,\n",
    "    'cust_sensitive': 4,\n",
    "    'cust_industrial': 3,\n",
    "    'cust_residential': 2,\n",
    "    'cust_commercial': 1\n",
    "}\n",
    "\n",
    "gdf_combined['psps_risk'] = (\n",
    "    weights['cust_lifesupport'] * (gdf_combined['cust_lifesupport'] + gdf_combined['downstream_cust_lifesupport']) +\n",
    "    weights['cust_medicalcert'] * (gdf_combined['cust_medicalcert'] + gdf_combined['downstream_cust_medicalcert']) +\n",
    "    weights['cust_urgent'] * (gdf_combined['cust_urgent'] + gdf_combined['downstream_cust_urgent']) +\n",
    "    weights['cust_sensitive'] * (gdf_combined['cust_sensitive'] + gdf_combined['downstream_cust_sensitive']) +\n",
    "    weights['cust_industrial'] * (gdf_combined['cust_industrial'] + gdf_combined['downstream_cust_industrial']) +\n",
    "    weights['cust_residential'] * (gdf_combined['cust_residential'] + gdf_combined['downstream_cust_residential']) +\n",
    "    weights['cust_commercial'] * (gdf_combined['cust_commercial'] + gdf_combined['downstream_cust_commercial'])\n",
    ")\n",
    "\n",
    "# Rank polygons by risk: rank 1 = highest risk.\n",
    "gdf_combined['risk_rank'] = gdf_combined['psps_risk'].rank(method='min', ascending=False).astype(int)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Create the Folium Map using heatmap styling (based on risk rank)\n",
    "# ---------------------------\n",
    "gdf_combined_bg = gdf_combined[gdf_combined[\"layer\"] == \"Background\"]\n",
    "gdf_combined_places = gdf_combined[gdf_combined[\"layer\"] == \"Incorporated Places\"]\n",
    "\n",
    "min_rank = gdf_combined['risk_rank'].min()  # typically 1\n",
    "max_rank = gdf_combined['risk_rank'].max()\n",
    "colormap = cm.LinearColormap(colors=[\"red\", \"green\"], vmin=min_rank, vmax=max_rank)\n",
    "colormap.caption = \"PSPS Risk Rank (1 = Highest Risk)\"\n",
    "\n",
    "center = [gdf_sd_places.geometry.unary_union.centroid.y,\n",
    "          gdf_sd_places.geometry.unary_union.centroid.x]\n",
    "m = folium.Map(location=center, zoom_start=10)\n",
    "colormap.add_to(m)\n",
    "\n",
    "background_tooltip = folium.GeoJsonTooltip(\n",
    "    fields=['NAME', 'cust_total', 'downstream_cust_total', 'risk_rank'],\n",
    "    aliases=['Section:', 'Customer Total:', 'Downstream Total:', 'Population Risk Rank:'],\n",
    "    localize=True\n",
    ")\n",
    "\n",
    "folium.GeoJson(\n",
    "    gdf_combined_bg.to_json(),\n",
    "    name='Background Polygons',\n",
    "    tooltip=background_tooltip,\n",
    "    style_function=risk_style\n",
    ").add_to(m)\n",
    "\n",
    "places_tooltip = folium.GeoJsonTooltip(\n",
    "    fields=['NAME_left', 'cust_total', 'downstream_cust_total', 'risk_rank'],\n",
    "    aliases=['Place:', 'Customer Total:', 'Downstream Total:', 'Population Risk Rank:'],\n",
    "    localize=True\n",
    ")\n",
    "\n",
    "folium.GeoJson(\n",
    "    gdf_combined_places.to_json(),\n",
    "    name='Incorporated Places',\n",
    "    tooltip=places_tooltip,\n",
    "    style_function=risk_style\n",
    ").add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Save the accumulated polygon data to CSV\n",
    "# ---------------------------\n",
    "gdf_combined[\"geometry\"] = gdf_combined[\"geometry\"].apply(lambda x: x.wkt)\n",
    "gdf_combined.to_csv(\"accumulated_nonoverlap_polygon_values.csv\", index=False)\n",
    "\n",
    "m.save(\"nonoverlap_polygons_map.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life = life[['NAME', 'NAME_left', 'geometry', 'layer', 'psps_risk', 'risk_rank']]\n",
    "life['combined_name'] = life['NAME'].fillna(life['NAME_left'])\n",
    "life.drop(columns=['NAME', 'NAME_left'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df = pd.read_csv(\"/mnt/data/Energy_Output.csv\")          # expected columns: latitude, longitude, energy_score\n",
    "weather_df = pd.read_csv(\"/mnt/data/weather_predictions.csv\")    # expected columns: latitude, longitude, weather_score\n",
    "life_risk_df = pd.read_csv(\"/mnt/data/life_risk.csv\")  \n",
    "\n",
    "#Subdiv Polygons\n",
    "subdiv_url = \"https://www2.census.gov/geo/tiger/TIGER2020/COUSUB/tl_2020_06_cousub.zip\"\n",
    "gdf_subdiv = gpd.read_file(subdiv_url)\n",
    "gdf_sd = gdf_subdiv[gdf_subdiv['COUNTYFP'] == '073']\n",
    "\n",
    "\n",
    "places_url = \"https://www2.census.gov/geo/tiger/TIGER2020/PLACE/tl_2020_06_place.zip\"\n",
    "gdf_places = gpd.read_file(places_url)\n",
    "counties_url = \"https://www2.census.gov/geo/tiger/TIGER2020/COUNTY/tl_2020_us_county.zip\"\n",
    "gdf_counties = gpd.read_file(counties_url)\n",
    "gdf_sd_county = gdf_counties[gdf_counties['NAME'] == 'San Diego']\n",
    "gdf_places = gdf_places.to_crs(\"EPSG:4326\")\n",
    "gdf_sd_county = gdf_sd_county.to_crs(\"EPSG:4326\")\n",
    "gdf_sd_places = gpd.sjoin(gdf_places, gdf_sd_county, how=\"inner\", predicate=\"intersects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_gpd = gpd.GeoDataFrame(energy, \n",
    "                                geometry=gpd.points_from_xy(energy['longitude'], energy['latitude']),\n",
    "                                crs='EPSG:4326')\n",
    "\n",
    "nature_gpd = gpd.GeoDataFrame(nature, \n",
    "                                geometry=gpd.points_from_xy(nature['longitude'], nature['latitude']),\n",
    "                                crs='EPSG:4326')\n",
    "weather_gpd = gpd.GeoDataFrame(weather, \n",
    "                                geometry=gpd.points_from_xy(weather['Longitude'], weather['Latitude']),\n",
    "                                crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random risk score\n",
    "def custom_overall_risk(row):\n",
    "    energy = row['avg_risk_gdf1']\n",
    "    nature = row['avg_risk_gdf2']\n",
    "    weather = row['avg_risk_gdf3']\n",
    "    return 0.4 * energy + 0.3 * nature + 0.3 * weather\n",
    "\n",
    "def calculate_avg_risk(polygon_gdf, risk_gdf, risk_field, suffix):\n",
    "    \"\"\"\n",
    "    Performs a spatial join of polygon_gdf with risk_gdf,\n",
    "    calculates the average risk score (risk_field) for each polygon,\n",
    "    and returns a Series indexed by the polygon's index.\n",
    "    \"\"\"\n",
    "    if 'index_right' in polygon_gdf.columns:\n",
    "        polygon_gdf = polygon_gdf.drop(columns=['index_right'])\n",
    "    if 'index_right' in risk_gdf.columns:\n",
    "        risk_gdf = risk_gdf.drop(columns=['index_right'])\n",
    "    \n",
    "    joined = gpd.sjoin(polygon_gdf, risk_gdf, predicate='contains', how='left')\n",
    "    # Group by the polygon's index (sjoin returns it as 'index_left')\n",
    "    avg_risk = joined.groupby(joined.index)[risk_field].mean().rename(f'avg_risk_{suffix}')\n",
    "    return avg_risk\n",
    "\n",
    "# ---- Incorporated Places ----\n",
    "for col in ['avg_risk_gdf1', 'avg_risk_gdf2', 'avg_risk_gdf3']:\n",
    "    if col in gdf_sd_places.columns:\n",
    "        gdf_sd_places.drop(columns=[col], inplace=True)\n",
    "\n",
    "avg_risk_1_places = calculate_avg_risk(gdf_sd_places, energy_gpd, 'probability (%)', 'gdf1')\n",
    "avg_risk_2_places = calculate_avg_risk(gdf_sd_places, nature_gpd, 'nature_index', 'gdf2')\n",
    "avg_risk_3_places = calculate_avg_risk(gdf_sd_places, weather_gpd, 'Predicted_Fire_Risk', 'gdf3')\n",
    "\n",
    "gdf_sd_places = gdf_sd_places.join(avg_risk_1_places, how='left', rsuffix='_new1')\n",
    "gdf_sd_places = gdf_sd_places.join(avg_risk_2_places, how='left', rsuffix='_new2')\n",
    "gdf_sd_places = gdf_sd_places.join(avg_risk_3_places, how='left', rsuffix='_new3')\n",
    "\n",
    "for col in ['avg_risk_gdf1', 'avg_risk_gdf2', 'avg_risk_gdf3']:\n",
    "    disp_col = col + '_disp'\n",
    "    gdf_sd_places[disp_col] = gdf_sd_places[col].apply(lambda x: f\"{x} (missing)\" if pd.isna(x) else x)\n",
    "    gdf_sd_places[col] = gdf_sd_places[col].fillna(0)\n",
    "\n",
    "gdf_sd_places['overall_risk'] = gdf_sd_places.apply(custom_overall_risk, axis=1)\n",
    "\n",
    "# ---- Background Polygons ----\n",
    "for col in ['avg_risk_gdf1', 'avg_risk_gdf2', 'avg_risk_gdf3']:\n",
    "    if col in gdf_sd.columns:\n",
    "        gdf_sd.drop(columns=[col], inplace=True)\n",
    "\n",
    "avg_risk_1_sd = calculate_avg_risk(gdf_sd, energy_gpd, 'probability (%)', 'gdf1')\n",
    "avg_risk_2_sd = calculate_avg_risk(gdf_sd, nature_gpd, 'nature_index', 'gdf2')\n",
    "avg_risk_3_sd = calculate_avg_risk(gdf_sd, weather_gpd, 'Predicted_Fire_Risk', 'gdf3')\n",
    "\n",
    "gdf_sd = gdf_sd.join(avg_risk_1_sd).join(avg_risk_2_sd).join(avg_risk_3_sd)\n",
    "\n",
    "for col in ['avg_risk_gdf1', 'avg_risk_gdf2', 'avg_risk_gdf3']:\n",
    "    disp_col = col + '_disp'\n",
    "    gdf_sd[disp_col] = gdf_sd[col].apply(lambda x: f\"{x} (missing)\" if pd.isna(x) else x)\n",
    "    gdf_sd[col] = gdf_sd[col].fillna(0)\n",
    "\n",
    "gdf_sd['overall_risk'] = gdf_sd.apply(custom_overall_risk, axis=1)\n",
    "\n",
    "places_union = unary_union(gdf_sd_places.geometry)\n",
    "gdf_sd['geometry'] = gdf_sd.geometry.apply(lambda geom: geom.difference(places_union))\n",
    "\n",
    "# Remove any duplicated columns before converting to JSON\n",
    "gdf_sd_places = gdf_sd_places.loc[:, ~gdf_sd_places.columns.duplicated()]\n",
    "gdf_sd = gdf_sd.loc[:, ~gdf_sd.columns.duplicated()]\n",
    "\n",
    "from branca.colormap import LinearColormap\n",
    "\n",
    "# Compute global min and max risk from both GeoDataFrames\n",
    "global_min_risk = min(gdf_sd['overall_risk'].min(), gdf_sd_places['overall_risk'].min())\n",
    "global_max_risk = max(gdf_sd['overall_risk'].max(), gdf_sd_places['overall_risk'].max())\n",
    "if pd.isna(global_min_risk) or global_min_risk is None:\n",
    "    global_min_risk = 0\n",
    "if pd.isna(global_max_risk) or global_max_risk is None:\n",
    "    global_max_risk = 1\n",
    "\n",
    "colormap = LinearColormap(['green', 'red'], vmin=global_min_risk, vmax=global_max_risk)\n",
    "colormap.caption = 'Overall Risk Heatmap'\n",
    "\n",
    "def heat_style(feature, fill_opacity=0.5):\n",
    "    risk = feature['properties'].get('overall_risk')\n",
    "    # If risk is missing or zero, color it gray\n",
    "    if risk is None or pd.isna(risk) or risk == 0:\n",
    "        fillColor = 'gray'\n",
    "    else:\n",
    "        fillColor = colormap(risk)\n",
    "    return {\n",
    "        'fillColor': fillColor,\n",
    "        'color': 'black',\n",
    "        'weight': 2,\n",
    "        'fillOpacity': fill_opacity\n",
    "    }\n",
    "\n",
    "center = [gdf_sd_places.geometry.unary_union.centroid.y,\n",
    "          gdf_sd_places.geometry.unary_union.centroid.x]\n",
    "m = folium.Map(location=center, zoom_start=10)\n",
    "colormap.add_to(m) \n",
    "\n",
    "# ---- Background Polygons ----\n",
    "gdf_sd = gdf_sd.loc[:, ~gdf_sd.columns.duplicated()]\n",
    "\n",
    "background_tooltip = folium.GeoJsonTooltip(\n",
    "    fields=['NAME', 'avg_risk_gdf1_disp', 'avg_risk_gdf2_disp', 'avg_risk_gdf3_disp', 'overall_risk'],\n",
    "    aliases=['Section:', 'Energy Risk:', 'Nature Risk:', 'Weather Risk:', 'Overall Risk:'],\n",
    "    localize=True\n",
    ")\n",
    "\n",
    "folium.GeoJson(\n",
    "    gdf_sd.to_json(),\n",
    "    name='Background Polygons',\n",
    "    tooltip=background_tooltip,\n",
    "    style_function=lambda feature: heat_style(feature, fill_opacity=0.5)\n",
    ").add_to(m)\n",
    "\n",
    "# ---- Incorporated Places ----\n",
    "gdf_sd_places = gdf_sd_places.loc[:, ~gdf_sd_places.columns.duplicated()]\n",
    "\n",
    "places_tooltip = folium.GeoJsonTooltip(\n",
    "    fields=['NAME_left', 'avg_risk_gdf1_disp', 'avg_risk_gdf2_disp', 'avg_risk_gdf3_disp', 'overall_risk'],\n",
    "    aliases=['Place:', 'Energy Risk:', 'Nature Risk:', 'Weather Risk:', 'Overall Risk:'],\n",
    "    localize=True\n",
    ")\n",
    "\n",
    "folium.GeoJson(\n",
    "    gdf_sd_places.to_json(),\n",
    "    name='Incorporated Places',\n",
    "    tooltip=places_tooltip,\n",
    "    style_function=lambda feature: heat_style(feature, fill_opacity=0.7)\n",
    ").add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save(\"alpha_map.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = pd.read_csv('outputs/Energy_Output.csv').drop(columns=['Unnamed: 0'])\n",
    "nature = pd.read_csv('outputs/nature_index_no_hftd.csv')\n",
    "weather = pd.read_csv('outputs/weather_predictions.csv')\n",
    "life = pd.read_csv('life_risk.csv')\n",
    "\n",
    "#Subdiv Polygons\n",
    "subdiv_url = \"https://www2.census.gov/geo/tiger/TIGER2020/COUSUB/tl_2020_06_cousub.zip\"\n",
    "gdf_subdiv = gpd.read_file(subdiv_url)\n",
    "gdf_sd = gdf_subdiv[gdf_subdiv['COUNTYFP'] == '073']\n",
    "\n",
    "\n",
    "places_url = \"https://www2.census.gov/geo/tiger/TIGER2020/PLACE/tl_2020_06_place.zip\"\n",
    "gdf_places = gpd.read_file(places_url)\n",
    "counties_url = \"https://www2.census.gov/geo/tiger/TIGER2020/COUNTY/tl_2020_us_county.zip\"\n",
    "gdf_counties = gpd.read_file(counties_url)\n",
    "gdf_sd_county = gdf_counties[gdf_counties['NAME'] == 'San Diego']\n",
    "gdf_places = gdf_places.to_crs(\"EPSG:4326\")\n",
    "gdf_sd_county = gdf_sd_county.to_crs(\"EPSG:4326\")\n",
    "gdf_sd_places = gpd.sjoin(gdf_places, gdf_sd_county, how=\"inner\", predicate=\"intersects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define capping threshold (e.g., 95th percentile)\n",
    "upper_cap = life[\"psps_risk\"].quantile(0.95)  # Adjust threshold if needed\n",
    "lower_cap = life[\"psps_risk\"].quantile(0)  # Optional lower cap if there are small outliers\n",
    "\n",
    "# Clip values to avoid extreme outliers\n",
    "life[\"psps_risk_capped\"] = np.clip(life[\"psps_risk\"], lower_cap, upper_cap)\n",
    "\n",
    "# Scale using the new min/max\n",
    "min_risk = life[\"psps_risk_capped\"].min()\n",
    "max_risk = life[\"psps_risk_capped\"].max()\n",
    "life[\"psps_risk_scaled\"] = ((life[\"psps_risk_capped\"] - min_risk) / (max_risk - min_risk)) * 100\n",
    "\n",
    "# Display results\n",
    "life[[\"psps_risk\", \"psps_risk_capped\", \"psps_risk_scaled\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_cap = weather[\"Predicted_Fire_Risk\"].quantile(0.95)  # Adjust threshold if needed\n",
    "lower_cap = weather[\"Predicted_Fire_Risk\"].quantile(0)  # Optional lower cap if there are small outliers\n",
    "\n",
    "# Clip values to avoid extreme outliers\n",
    "weather[\"Predicted_Fire_Risk_capped\"] = np.clip(weather[\"Predicted_Fire_Risk\"], lower_cap, upper_cap)\n",
    "\n",
    "# Scale using the new min/max\n",
    "min_risk = weather[\"Predicted_Fire_Risk_capped\"].min()\n",
    "max_risk = weather[\"Predicted_Fire_Risk_capped\"].max()\n",
    "weather[\"Predicted_Fire_Risk_scaled\"] = ((weather[\"Predicted_Fire_Risk_capped\"] - min_risk) / (max_risk - min_risk)) * 100\n",
    "\n",
    "# Display results\n",
    "weather[[\"Predicted_Fire_Risk\", \"Predicted_Fire_Risk_capped\", \"Predicted_Fire_Risk_scaled\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_cap = nature[\"nature_index\"].quantile(0.95)  # Adjust threshold if needed\n",
    "lower_cap = nature[\"nature_index\"].quantile(0.05)  # Optional lower cap if there are small outliers\n",
    "\n",
    "# Clip values to avoid extreme outliers\n",
    "nature[\"nature_index_capped\"] = np.clip(nature[\"nature_index\"], lower_cap, upper_cap)\n",
    "\n",
    "# Scale using the new min/max\n",
    "min_risk = nature[\"nature_index_capped\"].min()\n",
    "max_risk = nature[\"nature_index_capped\"].max()\n",
    "nature[\"nature_index_scaled\"] = ((nature[\"nature_index_capped\"] - min_risk) / (max_risk - min_risk)) * 100\n",
    "\n",
    "# Display results\n",
    "nature[[\"nature_index\", \"nature_index_capped\", \"nature_index_scaled\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['Predicted_Fire_Risk'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_gpd = gpd.GeoDataFrame(energy, \n",
    "                                geometry=gpd.points_from_xy(energy['longitude'], energy['latitude']),\n",
    "                                crs='EPSG:4326')\n",
    "\n",
    "nature_gpd = gpd.GeoDataFrame(nature, \n",
    "                                geometry=gpd.points_from_xy(nature['longitude'], nature['latitude']),\n",
    "                                crs='EPSG:4326')\n",
    "weather_gpd = gpd.GeoDataFrame(weather, \n",
    "                                geometry=gpd.points_from_xy(weather['longitude'], weather['latitude']),\n",
    "                                crs='EPSG:4326')\n",
    "\n",
    "life['geometry'] = life['geometry'].apply(wkt.loads)\n",
    "life_gdf = gpd.GeoDataFrame(life, geometry='geometry', crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "energy = pd.read_csv('outputs/Energy_Output.csv').drop(columns=['Unnamed: 0'])\n",
    "nature = pd.read_csv('outputs/nature_index_no_hftd.csv')\n",
    "weather = pd.read_csv('outputs/weather_predictions.csv')\n",
    "life = pd.read_csv('life_risk.csv')\n",
    "\n",
    "#Subdiv Polygons\n",
    "subdiv_url = \"https://www2.census.gov/geo/tiger/TIGER2020/COUSUB/tl_2020_06_cousub.zip\"\n",
    "gdf_subdiv = gpd.read_file(subdiv_url)\n",
    "gdf_sd = gdf_subdiv[gdf_subdiv['COUNTYFP'] == '073']\n",
    "\n",
    "\n",
    "places_url = \"https://www2.census.gov/geo/tiger/TIGER2020/PLACE/tl_2020_06_place.zip\"\n",
    "gdf_places = gpd.read_file(places_url)\n",
    "counties_url = \"https://www2.census.gov/geo/tiger/TIGER2020/COUNTY/tl_2020_us_county.zip\"\n",
    "gdf_counties = gpd.read_file(counties_url)\n",
    "gdf_sd_county = gdf_counties[gdf_counties['NAME'] == 'San Diego']\n",
    "gdf_places = gdf_places.to_crs(\"EPSG:4326\")\n",
    "gdf_sd_county = gdf_sd_county.to_crs(\"EPSG:4326\")\n",
    "gdf_sd_places = gpd.sjoin(gdf_places, gdf_sd_county, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "upper_cap = life[\"psps_risk\"].quantile(0.95)  # Adjust threshold if needed\n",
    "lower_cap = life[\"psps_risk\"].quantile(0)  # Optional lower cap if there are small outliers\n",
    "\n",
    "# Clip values to avoid extreme outliers\n",
    "life[\"psps_risk_capped\"] = np.clip(life[\"psps_risk\"], lower_cap, upper_cap)\n",
    "\n",
    "# Scale using the new min/max\n",
    "min_risk = life[\"psps_risk_capped\"].min()\n",
    "max_risk = life[\"psps_risk_capped\"].max()\n",
    "life[\"psps_risk_scaled\"] = ((life[\"psps_risk_capped\"] - min_risk) / (max_risk - min_risk)) * 100\n",
    "\n",
    "# Display results\n",
    "life[[\"psps_risk\", \"psps_risk_capped\", \"psps_risk_scaled\"]]\n",
    "\n",
    "import math\n",
    "\n",
    "def composite_risk(row):\n",
    "    \"\"\"\n",
    "    Calculate a composite risk score where the wildfire indicators\n",
    "    (Nature, Weather, Energy) battle against the Life factor, without using minimum values.\n",
    "\n",
    "    The formula is:\n",
    "    \n",
    "        composite = [ W_w * exp(W / λ)\n",
    "                    + W_n * exp(N / λ)\n",
    "                    + W_e * exp(E / λ) ]\n",
    "                    - g * (L^2)\n",
    "                    \n",
    "    where:\n",
    "      - W, N, E represent the weather, nature, and energy risk values.\n",
    "      - L represents the life factor (population or PSPS indicator).\n",
    "      - W_w, W_n, W_e are weights for each risk indicator.\n",
    "      - λ (lambda_) is an exponential scale parameter.\n",
    "      - g is the scaling factor for the life term.\n",
    "      \n",
    "    Adjust the parameters as needed based on your data scales.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract risk values from the row\n",
    "    W = row['avg_risk_gdf3']  # Weather risk\n",
    "    N = row['avg_risk_gdf2']  # Nature risk\n",
    "    E = row['avg_risk_gdf1']  # Energy risk\n",
    "    L = row['avg_risk_gdf4']  # Life factor (population)\n",
    "    \n",
    "    # --- Parameter initialization (tune these as needed) ---\n",
    "    W_w = 0.02  # Weight for weather risk\n",
    "    W_n = 0.03  # Weight for nature risk\n",
    "    W_e = 0.02  # Weight for energy risk\n",
    "    g   = 1.0  # Scaling factor for the life term (L^2)\n",
    "    \n",
    "    # Exponential scale parameter: controls the amplification of risk values\n",
    "    lambda_ = 1\n",
    "    \n",
    "    # --- Calculate the cumulative wildfire risk ---\n",
    "    wildfire_sum = (\n",
    "          W_w * math.exp(W / lambda_) +\n",
    "          W_n * math.exp(N / lambda_) +\n",
    "          W_e * math.exp(E / lambda_)\n",
    "    )\n",
    "    \n",
    "    # --- Composite risk: wildfire risk \"battles\" against life ---\n",
    "    composite = wildfire_sum - g * (L ** 2)\n",
    "    \n",
    "    return composite\n",
    "\n",
    "# Function to calculate the average risk for a given polygon layer and risk layer\n",
    "def calculate_avg_risk(polygon_gdf, risk_gdf, risk_field, suffix):\n",
    "    \"\"\"\n",
    "    Performs a spatial join of polygon_gdf with risk_gdf,\n",
    "    calculates the average risk score (risk_field) for each polygon,\n",
    "    and returns a Series indexed by the polygon's index.\n",
    "    \"\"\"\n",
    "    if 'index_right' in polygon_gdf.columns:\n",
    "        polygon_gdf = polygon_gdf.drop(columns=['index_right'])\n",
    "    if 'index_right' in risk_gdf.columns:\n",
    "        risk_gdf = risk_gdf.drop(columns=['index_right'])\n",
    "    \n",
    "    joined = gpd.sjoin(polygon_gdf, risk_gdf, predicate='contains', how='left')\n",
    "    # Group by the polygon's index (sjoin returns it as 'index_left')\n",
    "    avg_risk = joined.groupby(joined.index)[risk_field].mean().rename(f'avg_risk_{suffix}')\n",
    "    return avg_risk\n",
    "\n",
    "# ---- Incorporated Places ----\n",
    "# Remove risk columns if they exist to avoid duplicate names\n",
    "for col in ['avg_risk_gdf1', 'avg_risk_gdf2', 'avg_risk_gdf3']:\n",
    "    if col in gdf_sd_places.columns:\n",
    "        gdf_sd_places.drop(columns=[col], inplace=True)\n",
    "\n",
    "avg_risk_1_places = calculate_avg_risk(gdf_sd_places, energy_gpd, 'probability (%)', 'gdf1')\n",
    "avg_risk_2_places = calculate_avg_risk(gdf_sd_places, nature_gpd, 'nature_index', 'gdf2')\n",
    "avg_risk_3_places = calculate_avg_risk(gdf_sd_places, weather_gpd, 'Predicted_Fire_Risk', 'gdf3')\n",
    "avg_risk_4_places = calculate_avg_risk(gdf_sd_places, life_gdf, 'psps_risk_scaled', 'gdf4')\n",
    "\n",
    "gdf_sd_places = gdf_sd_places.join(avg_risk_1_places, how='left', rsuffix='_new1')\n",
    "gdf_sd_places = gdf_sd_places.join(avg_risk_2_places, how='left', rsuffix='_new2')\n",
    "gdf_sd_places = gdf_sd_places.join(avg_risk_3_places, how='left', rsuffix='_new3')\n",
    "gdf_sd_places = gdf_sd_places.join(avg_risk_4_places, how='left', rsuffix='_new3')\n",
    "\n",
    "# Create display columns for risk factors before filling NA\n",
    "for col in ['avg_risk_gdf1', 'avg_risk_gdf2', 'avg_risk_gdf3']:\n",
    "    disp_col = col + '_disp'\n",
    "    gdf_sd_places[disp_col] = gdf_sd_places[col].apply(lambda x: f\"{x} (missing)\" if pd.isna(x) else x)\n",
    "    gdf_sd_places[col] = gdf_sd_places[col].fillna(0)\n",
    "\n",
    "# ---- Background Polygons ----\n",
    "# Remove risk columns from gdf_sd if they exist\n",
    "for col in ['avg_risk_gdf1', 'avg_risk_gdf2', 'avg_risk_gdf3']:\n",
    "    if col in gdf_sd.columns:\n",
    "        gdf_sd.drop(columns=[col], inplace=True)\n",
    "\n",
    "avg_risk_1_sd = calculate_avg_risk(gdf_sd, energy_gpd, 'probability (%)', 'gdf1')\n",
    "avg_risk_2_sd = calculate_avg_risk(gdf_sd, nature_gpd, 'nature_index', 'gdf2')\n",
    "avg_risk_3_sd = calculate_avg_risk(gdf_sd, weather_gpd, 'Predicted_Fire_Risk', 'gdf3')\n",
    "avg_risk_4_sd = calculate_avg_risk(gdf_sd, life_gdf, 'psps_risk_scaled', 'gdf4')\n",
    "\n",
    "gdf_sd = gdf_sd.join(avg_risk_1_sd).join(avg_risk_2_sd).join(avg_risk_3_sd).join(avg_risk_4_sd)\n",
    "\n",
    "# Create display columns for risk factors before filling NA\n",
    "for col in ['avg_risk_gdf1', 'avg_risk_gdf2', 'avg_risk_gdf3']:\n",
    "    disp_col = col + '_disp'\n",
    "    gdf_sd[disp_col] = gdf_sd[col].apply(lambda x: f\"{x} (missing)\" if pd.isna(x) else x)\n",
    "    gdf_sd[col] = gdf_sd[col].fillna(0)\n",
    "\n",
    "for gdf in [gdf_sd_places, gdf_sd]:\n",
    "    col = 'avg_risk_gdf4'\n",
    "    disp_col = col + '_disp'\n",
    "    gdf[disp_col] = gdf[col].apply(lambda x: f\"{x} (missing)\" if pd.isna(x) else x)\n",
    "    gdf[col] = gdf[col].fillna(0)\n",
    "\n",
    "gdf_sd_places['overall_risk'] = gdf_sd_places.apply(composite_risk, axis=1)\n",
    "gdf_sd['overall_risk'] = gdf_sd.apply(composite_risk, axis=1)\n",
    "\n",
    "# Remove incorporated places from background polygons\n",
    "places_union = unary_union(gdf_sd_places.geometry)\n",
    "gdf_sd['geometry'] = gdf_sd.geometry.apply(lambda geom: geom.difference(places_union))\n",
    "\n",
    "# Remove any duplicated columns before converting to JSON\n",
    "gdf_sd_places = gdf_sd_places.loc[:, ~gdf_sd_places.columns.duplicated()]\n",
    "gdf_sd = gdf_sd.loc[:, ~gdf_sd.columns.duplicated()]\n",
    "\n",
    "from branca.colormap import LinearColormap\n",
    "\n",
    "gdf_sd = gdf_sd.reset_index(drop=True)\n",
    "gdf_sd_places = gdf_sd_places.reset_index(drop=True)\n",
    "\n",
    "# Combine overall risk values from both GeoDataFrames into one Series\n",
    "combined_risk = pd.concat([gdf_sd['overall_risk'], gdf_sd_places['overall_risk']], ignore_index=True)\n",
    "\n",
    "# Rank the combined series (lowest overall_risk gets rank 1)\n",
    "combined_rank = combined_risk.rank(method='min', ascending=True)\n",
    "\n",
    "# Assign the ranks back to each GeoDataFrame based on their positions in the combined series\n",
    "gdf_sd['composite_rank'] = combined_rank.iloc[:len(gdf_sd)].values\n",
    "gdf_sd_places['composite_rank'] = combined_rank.iloc[len(gdf_sd):].values\n",
    "\n",
    "# For the colormap, set the rank bounds:\n",
    "global_min_rank = 1\n",
    "global_max_rank = int(combined_rank.max())\n",
    "\n",
    "# Create a colormap that maps rank 1 (green) to rank global_max_rank (red)\n",
    "colormap = LinearColormap(['red', 'green'], vmin=global_min_rank, vmax=global_max_rank)\n",
    "colormap.caption = 'Composite Risk Rank'\n",
    "\n",
    "# Define a custom heat style function that handles missing values and zero risk:\n",
    "def heat_style(feature, fill_opacity=0.5):\n",
    "    # Retrieve the rank property\n",
    "    rank = feature['properties'].get('composite_rank')\n",
    "    if rank is None or pd.isna(rank):\n",
    "        fillColor = 'gray'\n",
    "    else:\n",
    "        fillColor = colormap(rank)\n",
    "    return {\n",
    "        'fillColor': fillColor,\n",
    "        'color': 'black',\n",
    "        'weight': 2,\n",
    "        'fillOpacity': fill_opacity\n",
    "    }\n",
    "\n",
    "# ---- Create the Folium Map ----\n",
    "center = [gdf_sd_places.geometry.unary_union.centroid.y,\n",
    "          gdf_sd_places.geometry.unary_union.centroid.x]\n",
    "m = folium.Map(location=center, zoom_start=10)\n",
    "colormap.add_to(m)  # Add colormap to the map\n",
    "\n",
    "# ---- Background Polygons ----\n",
    "gdf_sd = gdf_sd.loc[:, ~gdf_sd.columns.duplicated()]\n",
    "\n",
    "background_tooltip = folium.GeoJsonTooltip(\n",
    "    fields=['NAME', 'avg_risk_gdf1_disp', 'avg_risk_gdf2_disp', 'avg_risk_gdf3_disp', 'avg_risk_gdf4_disp', 'overall_risk', 'composite_rank'],\n",
    "    aliases=['Section:', 'Energy Risk:', 'Nature Risk:', 'Weather Risk:', 'Population Risk:', 'Overall Risk:', 'Rank:'],\n",
    "    localize=True\n",
    ")\n",
    "\n",
    "folium.GeoJson(\n",
    "    gdf_sd.to_json(),\n",
    "    name='Background Polygons',\n",
    "    tooltip=background_tooltip,\n",
    "    style_function=lambda feature: heat_style(feature, fill_opacity=0.5)\n",
    ").add_to(m)\n",
    "\n",
    "# ---- Incorporated Places ----\n",
    "gdf_sd_places = gdf_sd_places.loc[:, ~gdf_sd_places.columns.duplicated()]\n",
    "\n",
    "places_tooltip = folium.GeoJsonTooltip(\n",
    "    fields=['NAME_left', 'avg_risk_gdf1_disp', 'avg_risk_gdf2_disp', 'avg_risk_gdf3_disp', 'avg_risk_gdf4_disp', 'overall_risk', 'composite_rank'],\n",
    "    aliases=['Place:', 'Energy Risk:', 'Nature Risk:', 'Weather Risk:', 'Population Risk:', 'Overall Risk:', 'Rank:'],\n",
    "    localize=True\n",
    ")\n",
    "\n",
    "folium.GeoJson(\n",
    "    gdf_sd_places.to_json(),\n",
    "    name='Incorporated Places',\n",
    "    tooltip=places_tooltip,\n",
    "    style_function=lambda feature: heat_style(feature, fill_opacity=0.7)\n",
    ").add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Finally, display the map (in a Jupyter Notebook, simply output m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('result_viz.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.features import GeoJsonTooltip\n",
    "from shapely import wkt\n",
    "import matplotlib.pyplot as plt\n",
    "import branca.colormap as cm\n",
    "\n",
    "# ---- Define a style function based on risk rank ----\n",
    "# Here, lower rank means higher risk. We want rank 1 to be red and highest rank to be green.\n",
    "def risk_style(feature):\n",
    "    rank = feature['properties']['risk_rank']\n",
    "    return {\n",
    "        'fillColor': colormap(rank),\n",
    "        'color': 'black',\n",
    "        'weight': 1,\n",
    "        'fillOpacity': 0.6,\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load and prepare lines data\n",
    "# ---------------------------\n",
    "raw_df = pd.read_csv('data\\\\dev_wings_agg_span_2024_01_01.csv')\n",
    "df_lines = raw_df[['shape', 'cust_industrial', 'cust_commercial', 'cust_residential', 'cust_sensitive',\n",
    "                     'cust_essential', 'cust_medicalcert', 'cust_urgent', 'cust_lifesupport', 'cust_total',\n",
    "                     'downstream_cust_industrial', 'downstream_cust_commercial', 'downstream_cust_residential',\n",
    "                     'downstream_cust_sensitive', 'downstream_cust_essential', 'downstream_cust_medicalcert',\n",
    "                     'downstream_cust_urgent', 'downstream_cust_lifesupport', 'downstream_cust_total']]\n",
    "\n",
    "# Convert the 'shape' column (WKT strings) to geometry objects\n",
    "df_lines[\"geometry\"] = df_lines[\"shape\"].apply(wkt.loads)\n",
    "\n",
    "# Set the initial CRS (assumed EPSG:2230) then reproject to EPSG:4326 for Folium\n",
    "gdf_lines = gpd.GeoDataFrame(df_lines, geometry=\"geometry\", crs=\"EPSG:2230\")\n",
    "gdf_lines = gdf_lines.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load and prepare polygon data\n",
    "# ---------------------------\n",
    "# Load subdivisions (background polygons)\n",
    "subdiv_url = \"https://www2.census.gov/geo/tiger/TIGER2020/COUSUB/tl_2020_06_cousub.zip\"\n",
    "gdf_subdiv = gpd.read_file(subdiv_url)\n",
    "gdf_sd = gdf_subdiv[gdf_subdiv['COUNTYFP'] == '073']\n",
    "# Reproject background subdivisions to EPSG:4326\n",
    "gdf_sd = gdf_sd.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Load incorporated places and counties\n",
    "places_url = \"https://www2.census.gov/geo/tiger/TIGER2020/PLACE/tl_2020_06_place.zip\"\n",
    "gdf_places = gpd.read_file(places_url)\n",
    "counties_url = \"https://www2.census.gov/geo/tiger/TIGER2020/COUNTY/tl_2020_us_county.zip\"\n",
    "gdf_counties = gpd.read_file(counties_url)\n",
    "gdf_sd_county = gdf_counties[gdf_counties['NAME'] == 'San Diego']\n",
    "\n",
    "# Reproject places and county to EPSG:4326\n",
    "gdf_places = gdf_places.to_crs(\"EPSG:4326\")\n",
    "gdf_sd_county = gdf_sd_county.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Spatial join: Get incorporated places within San Diego County\n",
    "gdf_sd_places = gpd.sjoin(gdf_places, gdf_sd_county, how=\"inner\", predicate=\"intersects\")\n",
    "if 'index_right' in gdf_sd_places.columns:\n",
    "    gdf_sd_places = gdf_sd_places.drop(columns=['index_right'])\n",
    "gdf_sd_places = gdf_sd_places.loc[:, ~gdf_sd_places.columns.duplicated()]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Create non-overlapping background polygons\n",
    "# ---------------------------\n",
    "# Compute the union of the incorporated places\n",
    "places_union = gdf_sd_places.unary_union\n",
    "\n",
    "def subtract_places(geom, union_geom):\n",
    "    return geom.difference(union_geom)\n",
    "\n",
    "gdf_bg_nonoverlap = gdf_sd.copy()\n",
    "gdf_bg_nonoverlap[\"geometry\"] = gdf_bg_nonoverlap.geometry.apply(lambda g: subtract_places(g, places_union))\n",
    "gdf_bg_nonoverlap = gdf_bg_nonoverlap[~gdf_bg_nonoverlap.is_empty]\n",
    "gdf_bg_nonoverlap[\"layer\"] = \"Background\"\n",
    "gdf_sd_places[\"layer\"] = \"Incorporated Places\"\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Accumulate customer values into the non-overlapping polygons\n",
    "# ---------------------------\n",
    "gdf_combined = pd.concat([gdf_bg_nonoverlap, gdf_sd_places], ignore_index=True)\n",
    "\n",
    "# Spatial join: count any overlapping line (using \"intersects\")\n",
    "lines_in_polygons = gpd.sjoin(gdf_lines, gdf_combined, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "customer_fields = [\n",
    "    'cust_industrial', 'cust_commercial', 'cust_residential', 'cust_sensitive',\n",
    "    'cust_essential', 'cust_medicalcert', 'cust_urgent', 'cust_lifesupport', 'cust_total', \n",
    "    'downstream_cust_industrial', 'downstream_cust_commercial', 'downstream_cust_residential',\n",
    "    'downstream_cust_sensitive', 'downstream_cust_essential', 'downstream_cust_medicalcert',\n",
    "    'downstream_cust_urgent', 'downstream_cust_lifesupport', 'downstream_cust_total'\n",
    "]\n",
    "\n",
    "accumulated = lines_in_polygons.groupby(\"index_right\")[customer_fields].sum()\n",
    "gdf_combined = gdf_combined.merge(accumulated, left_index=True, right_index=True, how='left')\n",
    "gdf_combined[customer_fields] = gdf_combined[customer_fields].fillna(0)\n",
    "\n",
    "# ---- Compute PSPS Risk Score and Rank ----\n",
    "weights = {\n",
    "    'cust_lifesupport': 10,\n",
    "    'cust_medicalcert': 8,\n",
    "    'cust_urgent': 6,\n",
    "    'cust_sensitive': 4,\n",
    "    'cust_industrial': 3,\n",
    "    'cust_residential': 2,\n",
    "    'cust_commercial': 1\n",
    "}\n",
    "\n",
    "gdf_combined['psps_risk'] = (\n",
    "    weights['cust_lifesupport'] * (gdf_combined['cust_lifesupport'] + gdf_combined['downstream_cust_lifesupport']) +\n",
    "    weights['cust_medicalcert'] * (gdf_combined['cust_medicalcert'] + gdf_combined['downstream_cust_medicalcert']) +\n",
    "    weights['cust_urgent'] * (gdf_combined['cust_urgent'] + gdf_combined['downstream_cust_urgent']) +\n",
    "    weights['cust_sensitive'] * (gdf_combined['cust_sensitive'] + gdf_combined['downstream_cust_sensitive']) +\n",
    "    weights['cust_industrial'] * (gdf_combined['cust_industrial'] + gdf_combined['downstream_cust_industrial']) +\n",
    "    weights['cust_residential'] * (gdf_combined['cust_residential'] + gdf_combined['downstream_cust_residential']) +\n",
    "    weights['cust_commercial'] * (gdf_combined['cust_commercial'] + gdf_combined['downstream_cust_commercial'])\n",
    ")\n",
    "\n",
    "# Rank polygons by risk: rank 1 = highest risk.\n",
    "gdf_combined['risk_rank'] = gdf_combined['psps_risk'].rank(method='min', ascending=False).astype(int)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Create the Folium Map using heatmap styling (based on risk rank)\n",
    "# ---------------------------\n",
    "gdf_combined_bg = gdf_combined[gdf_combined[\"layer\"] == \"Background\"]\n",
    "gdf_combined_places = gdf_combined[gdf_combined[\"layer\"] == \"Incorporated Places\"]\n",
    "\n",
    "# Build a colormap based on risk_rank. Since rank 1 is highest risk, we want rank 1 to be red.\n",
    "min_rank = gdf_combined['risk_rank'].min()  # typically 1\n",
    "max_rank = gdf_combined['risk_rank'].max()\n",
    "colormap = cm.LinearColormap(colors=[\"red\", \"green\"], vmin=min_rank, vmax=max_rank)\n",
    "colormap.caption = \"PSPS Risk Rank (1 = Highest Risk)\"\n",
    "\n",
    "# Compute map center based on incorporated places’ union\n",
    "center = [gdf_sd_places.geometry.unary_union.centroid.y,\n",
    "          gdf_sd_places.geometry.unary_union.centroid.x]\n",
    "m = folium.Map(location=center, zoom_start=10)\n",
    "colormap.add_to(m)\n",
    "\n",
    "# ---- Background Polygons ----\n",
    "background_tooltip = folium.GeoJsonTooltip(\n",
    "    fields=['NAME', 'cust_total', 'downstream_cust_total', 'risk_rank'],\n",
    "    aliases=['Section:', 'Customer Total:', 'Downstream Total:', 'Population Risk Rank:'],\n",
    "    localize=True\n",
    ")\n",
    "\n",
    "folium.GeoJson(\n",
    "    gdf_combined_bg.to_json(),\n",
    "    name='Background Polygons',\n",
    "    tooltip=background_tooltip,\n",
    "    style_function=risk_style\n",
    ").add_to(m)\n",
    "\n",
    "# ---- Incorporated Places ----\n",
    "places_tooltip = folium.GeoJsonTooltip(\n",
    "    fields=['NAME_left', 'cust_total', 'downstream_cust_total', 'risk_rank'],\n",
    "    aliases=['Place:', 'Customer Total:', 'Downstream Total:', 'Population Risk Rank:'],\n",
    "    localize=True\n",
    ")\n",
    "\n",
    "folium.GeoJson(\n",
    "    gdf_combined_places.to_json(),\n",
    "    name='Incorporated Places',\n",
    "    tooltip=places_tooltip,\n",
    "    style_function=risk_style\n",
    ").add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Save the accumulated polygon data to CSV\n",
    "# ---------------------------\n",
    "gdf_combined[\"geometry\"] = gdf_combined[\"geometry\"].apply(lambda x: x.wkt)\n",
    "gdf_combined.to_csv(\"accumulated_nonoverlap_polygon_values.csv\", index=False)\n",
    "\n",
    "m.save(\"nonoverlap_polygons_map.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save(\"result_viz.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
