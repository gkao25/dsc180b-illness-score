{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Download all nc files from: https://sdge.sdsc.edu/data/sdge/historical-ens_gfs_004/portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "import xarray as xr\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed  \n",
    "import dask.array as da\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_urls = [\n",
    "    \"https://sdge.sdsc.edu/data/sdge/historical-ens_gfs_004/portal/202008-202107/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/historical-ens_gfs_004/portal/202108-202207/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/historical-ens_gfs_004/portal/202208-202307/\"\n",
    "]\n",
    "\n",
    "output_dir = os.getcwd()\n",
    "def download_nc_files(base_url):\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch the webpage: {base_url}. Status code: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    file_links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].endswith('.nc')]\n",
    "\n",
    "    for file_link in file_links:\n",
    "        file_url = base_url + file_link\n",
    "        file_name = os.path.join(output_dir, file_link)\n",
    "\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        file_response = requests.get(file_url)\n",
    "        if file_response.status_code == 200:\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(file_response.content)\n",
    "            print(f\"Saved: {file_name}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {file_url}. Status code: {file_response.status_code}\")\n",
    "\n",
    "for base_url in base_urls:\n",
    "    download_nc_files(base_url)\n",
    "\n",
    "print(\"All downloads completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat the nc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \".\"  \n",
    "nc_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".nc\")])\n",
    "\n",
    "\n",
    "batch_size = 100 # revise the batch size here \n",
    "total_batches = len(nc_files) // batch_size + (1 if len(nc_files) % batch_size else 0)\n",
    "\n",
    "print(f\"Total files: {len(nc_files)}, Creating {total_batches} batches.\")\n",
    "\n",
    "for batch_num, i in enumerate(range(0, len(nc_files), batch_size), start=1):\n",
    "    batch_files = nc_files[i : i + batch_size]\n",
    "    batch_filename = f\"size_100_batch_{batch_num}.nc\" # revise the batch size here as well\n",
    "\n",
    "    print(f\"\\nProcessing Batch {batch_num}/{total_batches} ({len(batch_files)} files)...\")\n",
    "    datasets = []\n",
    "\n",
    "    for file_idx, file in enumerate(batch_files, start=1):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        try:\n",
    "            ds = xr.open_dataset(file_path, chunks={})  \n",
    "            datasets.append(ds)\n",
    "            print(f\"{file_idx}/{len(batch_files)} concatenated...\") \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "    if not datasets:\n",
    "        print(f\"No valid datasets found in Batch {batch_num}, skipping...\")\n",
    "        continue \n",
    "\n",
    "    common_dims = set(datasets[0].dims)  \n",
    "    for ds in datasets:\n",
    "        common_dims.intersection_update(ds.dims) \n",
    "\n",
    "    exclude_dims = {\"x\", \"y\"}\n",
    "    valid_dims = [dim for dim in common_dims if dim not in exclude_dims]\n",
    "\n",
    "    if not valid_dims:\n",
    "        print(f\"No suitable common dimensions found for concatenation in Batch {batch_num}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    concat_dim = valid_dims[0]  \n",
    "    print(f\"Using '{concat_dim}' as the concatenation dimension.\")\n",
    "\n",
    "\n",
    "    combined_ds = xr.concat(datasets, dim=concat_dim)\n",
    "    with ProgressBar():\n",
    "        combined_ds.to_netcdf(batch_filename, engine=\"netcdf4\", encoding={concat_dim: {\"zlib\": True, \"complevel\": 4}})\n",
    "\n",
    "    print(f\"Batch {batch_num} saved: {batch_filename}\")\n",
    "\n",
    "print(\"\\nAll batches processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. EDA of Historical Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \".\" \n",
    "batch_files = sorted([f for f in os.listdir(folder_path) if f.startswith(\"size_100_batch_\") and f.endswith(\".nc\")])\n",
    "\n",
    "print(f\"Found {len(batch_files)} batch files.\")\n",
    "datasets = [xr.open_dataset(os.path.join(folder_path, file), chunks={}) for file in batch_files]\n",
    "common_dims = set(datasets[0].dims)\n",
    "for ds in datasets:\n",
    "    common_dims.intersection_update(ds.dims)\n",
    "\n",
    "exclude_dims = {\"x\", \"y\"}\n",
    "valid_dims = [dim for dim in common_dims if dim not in exclude_dims]\n",
    "\n",
    "if not valid_dims:\n",
    "    raise ValueError(\"No common dimension found for concatenation!\")\n",
    "\n",
    "concat_dim = valid_dims[0]  \n",
    "print(f\"Using '{concat_dim}' as the concatenation dimension.\")\n",
    "combined_ds = xr.concat(datasets, dim=concat_dim)\n",
    "\n",
    "print(\"\\nAvailable columns (variables) in the dataset:\")\n",
    "print(list(combined_ds.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['relative_humidity_2m', 'air_temperature_2m', 'heat_index', 'dew_point_temperature', \n",
    "    'eastward_10m_wind', 'northward_10m_wind', 'eastward_50m_wind', 'northward_50m_wind',\n",
    "    'surface_wind_gust', 'large_fire_potential_weather'\n",
    "]\n",
    "\n",
    "\n",
    "ds_selected = combined_ds[variables]\n",
    "\n",
    "sample_fraction = 0.20  \n",
    "total_points = ds_selected.dims['time']\n",
    "sample_size = int(total_points * sample_fraction)\n",
    "\n",
    "random_indices = np.random.choice(total_points, sample_size, replace=False)\n",
    "ds_sampled = ds_selected.isel(time=random_indices) \n",
    "\n",
    "ds_sampled = ds_sampled.to_array(dim=\"variable\")\n",
    "\n",
    "ds_values = ds_sampled.data \n",
    "ds_flattened = ds_values.reshape(len(ds_sampled), -1)  \n",
    "correlation_matrix = da.corrcoef(ds_flattened)  \n",
    "\n",
    "corr_df = pd.DataFrame(correlation_matrix.compute(), index=variables, columns=variables)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_df, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")  \n",
    "plt.yticks(rotation=0) \n",
    "\n",
    "plt.title(\"Correlation Matrix of Weather Variables (Sampled 20%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 EDA with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \".\" \n",
    "batch_files = sorted([f for f in os.listdir(folder_path) if f.startswith(\"size_100_batch_\") and f.endswith(\".nc\")])\n",
    "\n",
    "print(f\"Found {len(batch_files)} batch files.\")\n",
    "datasets = [xr.open_dataset(os.path.join(folder_path, file), chunks={}) for file in batch_files]\n",
    "common_dims = set(datasets[0].dims)\n",
    "for ds in datasets:\n",
    "    common_dims.intersection_update(ds.dims)\n",
    "\n",
    "exclude_dims = {\"x\", \"y\"}\n",
    "valid_dims = [dim for dim in common_dims if dim not in exclude_dims]\n",
    "\n",
    "if not valid_dims:\n",
    "    raise ValueError(\"No common dimension found for concatenation!\")\n",
    "\n",
    "concat_dim = valid_dims[0]  \n",
    "print(f\"Using '{concat_dim}' as the concatenation dimension.\")\n",
    "combined_ds = xr.concat(datasets, dim=concat_dim)\n",
    "\n",
    "print(\"\\nAvailable columns (variables) in the dataset:\")\n",
    "print(list(combined_ds.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    'relative_humidity_pbl', 'relative_humidity_low_trop', 'relative_humidity_mid_trop', \n",
    "    'relative_humidity_2m', 'air_temperature_2m', 'heat_index', 'dew_point_temperature', \n",
    "    'eastward_10m_wind', 'northward_10m_wind', 'eastward_50m_wind', 'northward_50m_wind',\n",
    "    'surface_wind_gust', 'large_fire_potential_weather'\n",
    "]\n",
    "\n",
    "ds_subset = combined_ds[variables]\n",
    "df_dask = ds_subset.to_dask_dataframe()\n",
    "df_dask = df_dask.dropna()\n",
    "df_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dask['wind_speed_10m'] = da.sqrt(df_dask['eastward_10m_wind']**2 + df_dask['northward_10m_wind']**2)\n",
    "df_dask['wind_speed_50m'] = da.sqrt(df_dask['eastward_50m_wind']**2 + df_dask['northward_50m_wind']**2)\n",
    "\n",
    "df_pandas = df_dask.compute()\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=df_pandas['relative_humidity_2m'], y=df_pandas['large_fire_potential_weather'], alpha=0.5)\n",
    "plt.xlabel('Relative Humidity at 2m (%)')\n",
    "plt.ylabel('Large Fire Potential Weather')\n",
    "plt.title('Humidity vs. Fire Potential')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=df_pandas['air_temperature_2m'], y=df_pandas['large_fire_potential_weather'], alpha=0.5, color='red')\n",
    "plt.xlabel('Air Temperature at 2m (K)')\n",
    "plt.ylabel('Large Fire Potential Weather')\n",
    "plt.title('Temperature vs. Fire Potential')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=df_pandas['wind_speed_10m'], y=df_pandas['large_fire_potential_weather'], alpha=0.5, color='green')\n",
    "plt.xlabel('Wind Speed at 10m (m/s)')\n",
    "plt.ylabel('Large Fire Potential Weather')\n",
    "plt.title('Wind Speed vs. Fire Potential')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df_pandas.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Weather Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_corr = correlation_matrix['large_fire_potential_weather'].abs().sort_values(ascending=False)\n",
    "print(\"\\nTop correlated features with 'large_fire_potential_weather':\")\n",
    "print(target_corr.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
