{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import unary_union\n",
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_nc_to_csv(file_path):\n",
    "    \"\"\"\n",
    "    Convert a .nc file to a CSV file with daily-averaged data filtered by a polygon.\n",
    "    \n",
    "    The CSV output is saved as:\n",
    "        Cleaned_ENS/YYYY-MM/YYYY-MM-DD.csv\n",
    "\n",
    "    Steps:\n",
    "      1. Extract an 8-digit date (YYYYMMDD) from the input file path and format it as YYYY-MM-DD.\n",
    "      2. Create a main output folder (\"Cleaned_ENS\") and a subfolder for the month (e.g., \"2020-08\").\n",
    "      3. Read the NetCDF file and compute daily means.\n",
    "      4. Convert the dataset to a DataFrame and create a \"lat_lon\" index.\n",
    "      5. Read the polygon from \"SD_gjson.json\" and filter rows where the point is within the polygon.\n",
    "      6. Write the filtered DataFrame to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the .nc file.\n",
    "    \"\"\"\n",
    "    # Extract date from file_path (expects an 8-digit date, e.g., 20200815)\n",
    "    match = re.search(r'(\\d{8})', file_path)\n",
    "    if not match:\n",
    "        raise ValueError(\"No valid date found in the file_path.\")\n",
    "    date_str = match.group(1)             # e.g., \"20200815\"\n",
    "    formatted_date = f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}\"  # \"2020-08-15\"\n",
    "    \n",
    "    # Create the main output folder and a subfolder (e.g., \"2020-08\")\n",
    "    main_folder = \"Cleaned_ENS\"\n",
    "    sub_folder = formatted_date[:7]  # \"YYYY-MM\"\n",
    "    output_folder = os.path.join(main_folder, sub_folder)\n",
    "    \n",
    "    if not os.path.exists(main_folder):\n",
    "        os.makedirs(main_folder)\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Define the output file name\n",
    "    output_file = os.path.join(output_folder, f\"{formatted_date}.csv\")\n",
    "    \n",
    "    # Open the dataset and compute daily means\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    ds_daily = ds.resample(time=\"1D\").mean()\n",
    "    \n",
    "    # Convert the dataset to a DataFrame and adjust the index\n",
    "    beta = ds_daily.to_dataframe().reset_index()\n",
    "    beta[\"lat_lon\"] = list(zip(beta[\"latitude\"], beta[\"longitude\"]))\n",
    "    beta = beta.set_index(\"lat_lon\")\n",
    "    beta = beta.drop(columns=[\"latitude\", \"longitude\"])\n",
    "    \n",
    "    # Read the polygon from the GeoJSON file and filter the DataFrame\n",
    "    sd_gdf = gpd.read_file(\"SD_gjson.json\")\n",
    "    sd_polygon = sd_gdf.unary_union\n",
    "    beta[\"point\"] = beta.index.map(lambda x: Point(x[1], x[0]))\n",
    "    df = beta[beta[\"point\"].apply(lambda pt: sd_polygon.contains(pt))].copy()\n",
    "    df = df.drop(columns=[\"point\"])\n",
    "\n",
    "    # List of fire-related columns\n",
    "    fire_cols = [\n",
    "        'energy_release_component', 'ignition_component', 'fire_intensity_level',\n",
    "        'forward_rate_of_spread', 'spread_component', 'burning_index', 'flame_length'\n",
    "    ]\n",
    "\n",
    "    for col in fire_cols:\n",
    "        df[col + '_norm'] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
    "\n",
    "    df['fire_risk_composite'] = df[[col + '_norm' for col in fire_cols]].mean(axis=1)\n",
    "\n",
    "    df['fire_risk_score'] = df['fire_risk_composite'] * 100\n",
    "\n",
    "    df['wind_speed'] = np.sqrt(df['eastward_10m_wind']**2 + df['northward_10m_wind']**2)\n",
    "\n",
    "    predictor_cols = [\n",
    "        'mean_wtd_moisture_1hr', 'mean_wtd_moisture_10hr', \n",
    "        'air_temperature_2m', 'air_relative_humidity_2m', \n",
    "        'wind_speed', 'accumulated_precipitation_amount',\n",
    "        'surface_downwelling_shortwave_flux'\n",
    "    ]\n",
    "\n",
    "    final_cols = predictor_cols + ['fire_risk_score']\n",
    "\n",
    "    df = df[final_cols]\n",
    "    \n",
    "    # Write the filtered DataFrame to CSV\n",
    "    df.to_csv(output_file)\n",
    "    print(f\"Filtered CSV written to: {output_file}\")\n",
    "\n",
    "#convert_nc_to_csv('ens_gfs_001/2020-08/dfmnfdrs_202008152000Z.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'ens_gfs_001/2020-08/dfmnfdrs_202008152000Z.nc'\n",
    "\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "ds_daily = ds.resample(time=\"1D\").mean()\n",
    "\n",
    "beta = ds_daily.to_dataframe().reset_index()\n",
    "\n",
    "beta[\"lat_lon\"] = list(zip(beta[\"latitude\"], beta[\"longitude\"]))\n",
    "\n",
    "beta = beta.set_index(\"lat_lon\")\n",
    "\n",
    "beta = beta.drop(columns=[\"latitude\", \"longitude\"])\n",
    "\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_gdf = gpd.read_file(\"SD_gjson.json\")\n",
    "sd_polygon = sd_gdf.unary_union\n",
    "beta[\"point\"] = beta.index.map(lambda x: Point(x[1], x[0]))\n",
    "df_filtered = beta[beta[\"point\"].apply(lambda pt: sd_polygon.contains(pt))].copy()\n",
    "df_filtered = df_filtered.drop(columns=[\"point\"])\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['south_north', 'west_east', 'YYYY', 'MM', 'DD', 'HH', 'mean_wtd_moisture_100hr', 'mean_wtd_moisture_1000hr',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = df_filtered\n",
    "\n",
    "# List of fire-related columns\n",
    "fire_cols = [\n",
    "    'energy_release_component', 'ignition_component', 'fire_intensity_level',\n",
    "    'forward_rate_of_spread', 'spread_component', 'burning_index', 'flame_length'\n",
    "]\n",
    "\n",
    "for col in fire_cols:\n",
    "    df[col + '_norm'] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
    "\n",
    "df['fire_risk_composite'] = df[[col + '_norm' for col in fire_cols]].mean(axis=1)\n",
    "\n",
    "df['fire_risk_score'] = df['fire_risk_composite'] * 100\n",
    "\n",
    "df['wind_speed'] = np.sqrt(df['eastward_10m_wind']**2 + df['northward_10m_wind']**2)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_cols = ['mean_wtd_moisture_1hr', 'mean_wtd_moisture_10hr', \n",
    "                  'air_temperature_2m', 'air_relative_humidity_2m', \n",
    "                  'wind_speed', 'accumulated_precipitation_amount',\n",
    "                  'surface_downwelling_shortwave_flux']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Drop rows with missing values (or you can choose to impute them)\n",
    "df_model = df.dropna(subset=predictor_cols + ['fire_risk_score'])\n",
    "\n",
    "# Split data into features and target\n",
    "X = df_model[predictor_cols]\n",
    "y = df_model['fire_risk_score']\n",
    "\n",
    "# Optionally, scale the predictors (important for some models)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a regression model (here, a random forest regressor)\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance (e.g., using RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_url = \"https://www2.census.gov/geo/tiger/TIGER2020/PLACE/tl_2020_06_place.zip\"\n",
    "gdf_places = gpd.read_file(places_url)\n",
    "\n",
    "counties_url = \"https://www2.census.gov/geo/tiger/TIGER2020/COUNTY/tl_2020_us_county.zip\"\n",
    "gdf_counties = gpd.read_file(counties_url)\n",
    "gdf_sd_county = gdf_counties[gdf_counties['NAME'] == 'San Diego']\n",
    "\n",
    "gdf_places = gdf_places.to_crs(\"EPSG:4326\")\n",
    "gdf_sd_county = gdf_sd_county.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Perform a spatial join: this will attach county info to places that intersect SD county\n",
    "gdf_sd_places = gpd.sjoin(gdf_places, gdf_sd_county, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "gdf_sd_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdiv_url = \"https://www2.census.gov/geo/tiger/TIGER2020/COUSUB/tl_2020_06_cousub.zip\"\n",
    "gdf_subdiv = gpd.read_file(subdiv_url)\n",
    "\n",
    "gdf_sd = gdf_subdiv[gdf_subdiv['COUNTYFP'] == '073']\n",
    "\n",
    "gdf_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "# --- Plot your points ---\n",
    "# Extract latitudes and longitudes from the DataFrame's index (assumed to be (lat, lon) tuples)\n",
    "lats = df_filtered.index.map(lambda x: x[0])\n",
    "lons = df_filtered.index.map(lambda x: x[1])\n",
    "center_lat = pd.Series(lats).mean()\n",
    "center_lon = pd.Series(lons).mean()\n",
    "\n",
    "# Create the map centered on your points\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=7)\n",
    "\n",
    "# Add each point as a red CircleMarker\n",
    "for lat, lon in zip(lats, lons):\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "        radius=3,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.7\n",
    "    ).add_to(m)\n",
    "\n",
    "# --- Add Polygon Layers ---\n",
    "# First, add the subdivisions layer (lower priority)\n",
    "folium.GeoJson(\n",
    "    gdf_sd.to_json(),\n",
    "    name=\"Subdivisions\",\n",
    "    style_function=lambda feature: {\n",
    "        \"fillColor\": \"lightgreen\",\n",
    "        \"color\": \"green\",\n",
    "        \"weight\": 2,\n",
    "        \"fillOpacity\": 0.3,\n",
    "    },\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=[\"NAME\"],\n",
    "        aliases=[\"Subdivision:\"]\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "# Next, add the incorporated places layer (higher priority)\n",
    "# Use \"NAME_left\" because that field exists in your gdf_sd_places\n",
    "folium.GeoJson(\n",
    "    gdf_sd_places.to_json(),\n",
    "    name=\"Incorporated Places\",\n",
    "    style_function=lambda feature: {\n",
    "        \"fillColor\": \"lightblue\",\n",
    "        \"color\": \"blue\",\n",
    "        \"weight\": 2,\n",
    "        \"fillOpacity\": 0.3,\n",
    "    },\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=[\"NAME_left\"],\n",
    "        aliases=[\"Place:\"]\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "# Add a layer control to toggle layers on/off\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Cleaned_ENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "csv_folder_path = \"Cleaned_ENS/2022-08\"\n",
    "\n",
    "csv_files = glob.glob(os.path.join(csv_folder_path, \"*.csv\"))\n",
    "\n",
    "df_list = []\n",
    "for file in csv_files:\n",
    "    temp_df = pd.read_csv(file)  # Adjust 'sep' if needed\n",
    "    df_list.append(temp_df)\n",
    "\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "combined_df[['latitude','longitude']] = (\n",
    "    combined_df['lat_lon']\n",
    "    .str.strip('()')           # remove parentheses\n",
    "    .str.split(',', expand=True)\n",
    "    .apply(lambda col: pd.to_numeric(col, errors='coerce'))\n",
    ")\n",
    "\n",
    "combined_df.drop('lat_lon', axis=1, inplace=True)\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "def combine(folder_path):\n",
    "    df_list = []\n",
    "\n",
    "    for subfolder in [f.path for f in os.scandir(folder_path) if f.is_dir()]:\n",
    "        start_time = time.time()\n",
    "        csv_files = glob.glob(os.path.join(subfolder, \"*.csv\"))\n",
    "        for file in csv_files:\n",
    "            temp_df = pd.read_csv(file)  # Adjust 'sep' if needed\n",
    "\n",
    "            # Process lat_lon into latitude and longitude\n",
    "            temp_df[['latitude', 'longitude']] = (\n",
    "                temp_df['lat_lon']\n",
    "                .str.strip('()')  # Remove parentheses\n",
    "                .str.split(',', expand=True)\n",
    "                .apply(lambda col: pd.to_numeric(col, errors='coerce'))\n",
    "            )\n",
    "\n",
    "            temp_df.drop('lat_lon', axis=1, inplace=True)\n",
    "\n",
    "            df_list.append(temp_df)\n",
    "        end_time = time.time()\n",
    "        print(f\"Execution time: {end_time - start_time:.4f} seconds\")\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = combine('Cleaned_ENS')\n",
    "total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "from torch_optimizer import Lamb, Ranger\n",
    "\n",
    "class HumidityAttentionMLP(nn.Module):\n",
    "    def __init__(self, num_other_features=8, embed_dim=128, num_heads=8, mlp_hidden=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.humidity_embedding = nn.Linear(1, embed_dim)\n",
    "        self.other_embedding = nn.Linear(1, embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden, mlp_hidden),  # Additional layer\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(mlp_hidden)\n",
    "        self.fc2 = nn.Linear(mlp_hidden, 1)\n",
    "\n",
    "    def forward(self, humidity, other_features):\n",
    "        humidity = humidity.unsqueeze(-1)\n",
    "        humidity_emb = self.humidity_embedding(humidity)\n",
    "        other_features = other_features.unsqueeze(-1)\n",
    "        other_emb = self.other_embedding(other_features)\n",
    "        attn_output, attn_weights = self.attention(humidity_emb, other_emb, other_emb)\n",
    "        attn_output = self.norm1(attn_output.squeeze(1))\n",
    "        x = self.fc1(attn_output)\n",
    "        x = self.norm2(x)\n",
    "        out = self.fc2(x)\n",
    "        return out, attn_weights\n",
    "\n",
    "# Define which columns will serve as the other features (key/value)\n",
    "other_features_cols = [\n",
    "    'mean_wtd_moisture_1hr', 'mean_wtd_moisture_10hr',\n",
    "    'air_temperature_2m', 'wind_speed',\n",
    "    'accumulated_precipitation_amount', 'surface_downwelling_shortwave_flux'\n",
    "]\n",
    "\n",
    "# Columns for latitude and longitude\n",
    "geo_features_cols = ['latitude', 'longitude']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into train (80%) and validation (20%)\n",
    "train_data, val_data = train_test_split(total_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize input features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_other = torch.tensor(scaler.fit_transform(train_data[other_features_cols].values), dtype=torch.float32)\n",
    "val_other = torch.tensor(scaler.transform(val_data[other_features_cols].values), dtype=torch.float32)\n",
    "\n",
    "# Keep latitude and longitude in their original form\n",
    "train_geo = torch.tensor(train_data[geo_features_cols].values, dtype=torch.float32)\n",
    "val_geo = torch.tensor(val_data[geo_features_cols].values, dtype=torch.float32)\n",
    "\n",
    "# Combine scaled features with unscaled latitude and longitude\n",
    "train_other = torch.cat([train_other, train_geo], dim=1)\n",
    "val_other = torch.cat([val_other, val_geo], dim=1)\n",
    "\n",
    "train_humidity = torch.tensor(scaler.fit_transform(train_data[['air_relative_humidity_2m']].values), dtype=torch.float32)\n",
    "val_humidity = torch.tensor(scaler.transform(val_data[['air_relative_humidity_2m']].values), dtype=torch.float32)\n",
    "\n",
    "# Normalize target labels\n",
    "train_labels = torch.tensor(scaler.fit_transform(train_data[['fire_risk_score']].values), dtype=torch.float32)\n",
    "val_labels = torch.tensor(scaler.transform(val_data[['fire_risk_score']].values), dtype=torch.float32)\n",
    "\n",
    "train_other[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "from torch_optimizer import Lamb, Ranger\n",
    "\n",
    "class HumidityAttentionMLP(nn.Module):\n",
    "    def __init__(self, num_other_features=8, embed_dim=128, num_heads=8, mlp_hidden=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.humidity_embedding = nn.Linear(1, embed_dim)\n",
    "        self.other_embedding = nn.Linear(1, embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden, mlp_hidden),  # Additional layer\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(mlp_hidden)\n",
    "        self.fc2 = nn.Linear(mlp_hidden, 1)\n",
    "\n",
    "    def forward(self, humidity, other_features):\n",
    "        humidity = humidity.unsqueeze(-1)\n",
    "        humidity_emb = self.humidity_embedding(humidity)\n",
    "        other_features = other_features.unsqueeze(-1)\n",
    "        other_emb = self.other_embedding(other_features)\n",
    "        attn_output, attn_weights = self.attention(humidity_emb, other_emb, other_emb)\n",
    "        attn_output = self.norm1(attn_output.squeeze(1))\n",
    "        x = self.fc1(attn_output)\n",
    "        x = self.norm2(x)\n",
    "        out = self.fc2(x)\n",
    "        return out, attn_weights\n",
    "\n",
    "# Define which columns will serve as the other features (key/value)\n",
    "other_features_cols = [\n",
    "    'mean_wtd_moisture_1hr', 'mean_wtd_moisture_10hr',\n",
    "    'air_temperature_2m', 'wind_speed',\n",
    "    'accumulated_precipitation_amount', 'surface_downwelling_shortwave_flux'\n",
    "]\n",
    "\n",
    "# Columns for latitude and longitude\n",
    "geo_features_cols = ['latitude', 'longitude']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into train (80%) and validation (20%)\n",
    "train_data, val_data = train_test_split(total_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize input features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_other = torch.tensor(scaler.fit_transform(train_data[other_features_cols].values), dtype=torch.float32)\n",
    "val_other = torch.tensor(scaler.transform(val_data[other_features_cols].values), dtype=torch.float32)\n",
    "\n",
    "# Keep latitude and longitude in their original form\n",
    "train_geo = torch.tensor(train_data[geo_features_cols].values, dtype=torch.float32)\n",
    "val_geo = torch.tensor(val_data[geo_features_cols].values, dtype=torch.float32)\n",
    "\n",
    "# Combine scaled features with unscaled latitude and longitude\n",
    "train_other = torch.cat([train_other, train_geo], dim=1)\n",
    "val_other = torch.cat([val_other, val_geo], dim=1)\n",
    "\n",
    "train_humidity = torch.tensor(scaler.fit_transform(train_data[['air_relative_humidity_2m']].values), dtype=torch.float32)\n",
    "val_humidity = torch.tensor(scaler.transform(val_data[['air_relative_humidity_2m']].values), dtype=torch.float32)\n",
    "\n",
    "# Normalize target labels\n",
    "train_labels = torch.tensor(scaler.fit_transform(train_data[['fire_risk_score']].values), dtype=torch.float32)\n",
    "val_labels = torch.tensor(scaler.transform(val_data[['fire_risk_score']].values), dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class FireRiskDataset(Dataset):\n",
    "    def __init__(self, humidity_tensor, other_tensor, label_tensor):\n",
    "        self.humidity = humidity_tensor\n",
    "        self.other_features = other_tensor\n",
    "        self.labels = label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.humidity[idx], self.other_features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = FireRiskDataset(train_humidity, train_other, train_labels)\n",
    "val_dataset = FireRiskDataset(val_humidity, val_other, val_labels)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Verify tensor shapes\n",
    "print(\"Humidity tensor shape:\", train_humidity.shape)  # (num_samples,)\n",
    "print(\"Other features tensor shape:\", train_other.shape)  # (num_samples, 8)\n",
    "print(\"Labels tensor shape:\", train_labels.shape)  # (num_samples, 1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Initialize the model and move it to GPU\n",
    "model = HumidityAttentionMLP()\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.HuberLoss()\n",
    "optimizer = Lamb(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "import time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Start timer for the entire epoch\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Initialize point counter and timer for the 1,000,000 point intervals\n",
    "    point_counter = 0\n",
    "    point_timer = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for batch_humidity, batch_other, batch_labels in train_loader:\n",
    "        # Move data to device\n",
    "        batch_humidity = batch_humidity.to(device)\n",
    "        batch_other = batch_other.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(batch_humidity, batch_other)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item() * batch_labels.size(0)\n",
    "        \n",
    "        # Update counter with number of points in the current batch\n",
    "        point_counter += batch_labels.size(0)\n",
    "        \n",
    "        # Check if we've processed 1,000,000 points\n",
    "        if point_counter >= 1_000_000:\n",
    "            points_elapsed_time = time.time() - point_timer\n",
    "            print(f\"Processed 1,000,000 points in {points_elapsed_time:.2f} seconds\")\n",
    "            # Reset the point timer and counter for the next interval\n",
    "            point_timer = time.time()\n",
    "            point_counter = 0\n",
    "\n",
    "    epoch_train_loss = total_train_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_humidity, batch_other, batch_labels in val_loader:\n",
    "            batch_humidity = batch_humidity.to(device)\n",
    "            batch_other = batch_other.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            outputs, _ = model(batch_humidity, batch_other)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            total_val_loss += loss.item() * batch_labels.size(0)\n",
    "\n",
    "    epoch_val_loss = total_val_loss / len(val_dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    # Check for early stopping\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "    # Step the scheduler after each epoch\n",
    "    scheduler.step()\n",
    "\n",
    "    # End of epoch: calculate and print total epoch time\n",
    "    epoch_elapsed_time = time.time() - epoch_start_time\n",
    "    print(f\"Epoch {epoch + 1} finished in {epoch_elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Select a single test sample (modify the index as needed)\n",
    "test_idx = 865356  # Change index as needed\n",
    "test_humidity, test_other, test_label = val_dataset[test_idx]\n",
    "\n",
    "# Move data to the correct device\n",
    "test_humidity = test_humidity.unsqueeze(0).to(device)  # Add batch dimension\n",
    "test_other = test_other.unsqueeze(0).to(device)  # Add batch dimension\n",
    "test_label = test_label.unsqueeze(0).to(device)\n",
    "\n",
    "# Disable gradient calculations for inference\n",
    "with torch.no_grad():\n",
    "    output, attn_weights = model(test_humidity, test_other)\n",
    "\n",
    "# Convert output and label to CPU NumPy arrays\n",
    "output_np = output.cpu().numpy()\n",
    "test_label_np = test_label.cpu().numpy()\n",
    "\n",
    "# Undo normalization (Inverse transform for scaled features)\n",
    "output_original = scaler.inverse_transform(output_np)  # Convert output back to original scale\n",
    "test_label_original = scaler.inverse_transform(test_label_np)  # Convert label back to original scale\n",
    "\n",
    "# Latitude and longitude are already in original scale\n",
    "test_geo_original = test_other[:, -2:].cpu().numpy()  # Extract latitude and longitude\n",
    "test_other_original = test_other.cpu().numpy()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Predicted fire risk score (original scale): {output_original}\")\n",
    "print(f\"Actual fire risk score (original scale): {test_label_original}\")\n",
    "print(f\"Latitude and Longitude (original scale): {test_geo_original}\")\n",
    "test_other_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'humidity_attention_mlp.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_nc_to_csv('dfmnfdrs_202502281200Z.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Cleaned_ENS/2025-02/2025-02-28.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "csv_folder_path = \"Cleaned_ENS/2025-02\"\n",
    "\n",
    "csv_files = glob.glob(os.path.join(csv_folder_path, \"*.csv\"))\n",
    "\n",
    "df_list = []\n",
    "for file in csv_files:\n",
    "    temp_df = pd.read_csv(file)  # Adjust 'sep' if needed\n",
    "    df_list.append(temp_df)\n",
    "\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "combined_df[['latitude','longitude']] = (\n",
    "    combined_df['lat_lon']\n",
    "    .str.strip('()')           # remove parentheses\n",
    "    .str.split(',', expand=True)\n",
    "    .apply(lambda col: pd.to_numeric(col, errors='coerce'))\n",
    ")\n",
    "\n",
    "combined_df.drop('lat_lon', axis=1, inplace=True)\n",
    "\n",
    "predict_df = combined_df\n",
    "\n",
    "predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features_cols = [\n",
    "    'mean_wtd_moisture_1hr', 'mean_wtd_moisture_10hr',\n",
    "    'air_temperature_2m', 'wind_speed',\n",
    "    'accumulated_precipitation_amount', 'surface_downwelling_shortwave_flux'\n",
    "]\n",
    "\n",
    "# Columns for latitude and longitude\n",
    "geo_features_cols = ['latitude', 'longitude']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "predict_other = torch.tensor(scaler.fit_transform(predict_df[other_features_cols].values), dtype=torch.float32)\n",
    "\n",
    "predict_humidity = torch.tensor(scaler.fit_transform(predict_df[['air_relative_humidity_2m']].values), dtype=torch.float32)\n",
    "\n",
    "predict_labels = torch.tensor(scaler.fit_transform(predict_df[['fire_risk_score']].values), dtype=torch.float32)\n",
    "\n",
    "predict_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireRiskDataset(Dataset):\n",
    "    def __init__(self, humidity_tensor, other_tensor, label_tensor):\n",
    "        self.humidity = humidity_tensor\n",
    "        self.other_features = other_tensor\n",
    "        self.labels = label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.humidity[idx], self.other_features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = FireRiskDataset(predict_humidity, predict_other, predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "other_features_cols = [\n",
    "    'mean_wtd_moisture_1hr', 'mean_wtd_moisture_10hr',\n",
    "    'air_temperature_2m', 'wind_speed',\n",
    "    'accumulated_precipitation_amount', 'surface_downwelling_shortwave_flux'\n",
    "]\n",
    "\n",
    "# Columns for latitude and longitude\n",
    "geo_features_cols = ['latitude', 'longitude']\n",
    "\n",
    "predict_other = torch.tensor(scaler.fit_transform(predict_df[other_features_cols].values), dtype=torch.float32)\n",
    "predict_geo = torch.tensor(predict_df[geo_features_cols].values, dtype=torch.float32)\n",
    "predict_other = torch.cat([predict_other, predict_geo], dim=1)\n",
    "\n",
    "predict_humidity = torch.tensor(scaler.fit_transform(predict_df[['air_relative_humidity_2m']].values), dtype=torch.float32)\n",
    "\n",
    "predict_labels = torch.tensor(scaler.fit_transform(predict_df[['fire_risk_score']].values), dtype=torch.float32)\n",
    "\n",
    "class FireRiskDataset(Dataset):\n",
    "    def __init__(self, humidity_tensor, other_tensor, label_tensor):\n",
    "        self.humidity = humidity_tensor\n",
    "        self.other_features = other_tensor\n",
    "        self.labels = label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.humidity[idx], self.other_features[idx], self.labels[idx]\n",
    "\n",
    "predict_dataset = FireRiskDataset(predict_humidity, predict_other, predict_labels)\n",
    "\n",
    "class HumidityAttentionMLP(nn.Module):\n",
    "    def __init__(self, num_other_features=8, embed_dim=128, num_heads=8, mlp_hidden=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.humidity_embedding = nn.Linear(1, embed_dim)\n",
    "        self.other_embedding = nn.Linear(1, embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden, mlp_hidden),  # Additional layer\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(mlp_hidden)\n",
    "        self.fc2 = nn.Linear(mlp_hidden, 1)\n",
    "\n",
    "    def forward(self, humidity, other_features):\n",
    "        humidity = humidity.unsqueeze(-1)\n",
    "        humidity_emb = self.humidity_embedding(humidity)\n",
    "        other_features = other_features.unsqueeze(-1)\n",
    "        other_emb = self.other_embedding(other_features)\n",
    "        attn_output, attn_weights = self.attention(humidity_emb, other_emb, other_emb)\n",
    "        attn_output = self.norm1(attn_output.squeeze(1))\n",
    "        x = self.fc1(attn_output)\n",
    "        x = self.norm2(x)\n",
    "        out = self.fc2(x)\n",
    "        return out, attn_weights\n",
    "\n",
    "# Load the saved state dictionary\n",
    "#model.load_state_dict(torch.load('models/humidity_attention_mlp.pth'))\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "predict_idx = 0\n",
    "predict_humidity, predict_other, predict_label = predict_dataset[predict_idx]\n",
    "\n",
    "# Move data to the correct device\n",
    "predict_humidity = predict_humidity.unsqueeze(0).to(device)  # Add batch dimension\n",
    "predict_other = predict_other.unsqueeze(0).to(device)  # Add batch dimension\n",
    "predict_label = predict_label.unsqueeze(0).to(device)\n",
    "\n",
    "# Disable gradient calculations for inference\n",
    "with torch.no_grad():\n",
    "    predict, attn_weights = model(predict_humidity, predict_other)\n",
    "\n",
    "print(output)\n",
    "# Convert output and label to CPU NumPy arrays\n",
    "predict_np = predict.cpu().numpy()\n",
    "preict_label_np = predict_label.cpu().numpy()\n",
    "\n",
    "# Undo normalization (Inverse transform for scaled features)\n",
    "predict_original = scaler.inverse_transform(predict_np)  # Convert output back to original scale\n",
    "predict_label_original = scaler.inverse_transform(preict_label_np)  # Convert label back to original scale\n",
    "\n",
    "# Latitude and longitude are already in original scale\n",
    "predict_geo_original = predict_other[:, -2:].cpu().numpy()  # Extract latitude and longitude\n",
    "\n",
    "# Print the results\n",
    "print(f\"Predicted fire risk score (original scale): {predict_original}\")\n",
    "print(f\"Actual fire risk score (original scale): {predict_label_original}\")\n",
    "print(f\"Latitude and Longitude (original scale): {predict_geo_original}\")\n",
    "\n",
    "output_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "results = []  # To store [latitude, longitude, predicted fire risk, actual fire risk]\n",
    "\n",
    "# Disable gradient calculations for inference\n",
    "with torch.no_grad():\n",
    "    for predict_humidity, predict_other, predict_label in predict_dataset:\n",
    "        # Add batch dimension and move data to device\n",
    "        predict_humidity = predict_humidity.unsqueeze(0).to(device)\n",
    "        predict_other = predict_other.unsqueeze(0).to(device)\n",
    "        predict_label = predict_label.unsqueeze(0).to(device)\n",
    "\n",
    "        # Run the model to get the prediction\n",
    "        predict, attn_weights = model(predict_humidity, predict_other)\n",
    "\n",
    "        # Convert predictions and labels to CPU NumPy arrays\n",
    "        predict_np = predict.cpu().numpy()\n",
    "        predict_label_np = predict_label.cpu().numpy()\n",
    "\n",
    "        # Undo normalization (inverse transform for scaled features)\n",
    "        predict_original = scaler.inverse_transform(predict_np)\n",
    "        predict_label_original = scaler.inverse_transform(predict_label_np)\n",
    "\n",
    "        # Extract latitude and longitude (assuming they are the last two features in predict_other)\n",
    "        predict_geo_original = predict_other[:, -2:].cpu().numpy()\n",
    "\n",
    "        # Extract values (assuming single value predictions/labels)\n",
    "        fire_risk_pred = predict_original[0, 0]\n",
    "        fire_risk_actual = predict_label_original[0, 0]\n",
    "        latitude = predict_geo_original[0, 0]\n",
    "        longitude = predict_geo_original[0, 1]\n",
    "\n",
    "        # Append the results as a row [latitude, longitude, predicted fire risk, actual fire risk]\n",
    "        results.append([latitude, longitude, fire_risk_pred])\n",
    "\n",
    "# Create a DataFrame with the results and specify the column names\n",
    "df_predictions = pd.DataFrame(results, columns=['Latitude', 'Longitude', 'Predicted_Fire_Risk'])\n",
    "\n",
    "# Save the DataFrame to a CSV file without the index column\n",
    "df_predictions.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(\"CSV file 'predictions.csv' has been created with predictions and actual labels.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
