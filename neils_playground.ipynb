{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading All Of ens_gfs_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def download_files_from_url(base_url, save_directory):\n",
    "    subfolder = base_url.split(\"/\")[-2]\n",
    "    subfolder_path = os.path.join(save_directory, subfolder)\n",
    "\n",
    "    # Ensure the subfolder exists\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        os.makedirs(subfolder_path)\n",
    "\n",
    "    # Fetch the HTML content from the URL\n",
    "    response = requests.get(base_url)\n",
    "    data = response.text\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    # Extract filenames from the table\n",
    "    fns = []\n",
    "    for tr in soup.find('table').find_all('tr'):\n",
    "        row = [url.text for url in tr.find_all('a')]\n",
    "        if row:  # Ensure the row is not empty\n",
    "            fns.append(row[1])\n",
    "\n",
    "    # The first 2 elements are not filenames\n",
    "    fns = fns[2:]\n",
    "\n",
    "    # Prepare the list of file URLs and destination paths\n",
    "    files = []\n",
    "    dest = []\n",
    "    for i in fns:\n",
    "        file_url = base_url + i\n",
    "        files.append(file_url)\n",
    "\n",
    "        dest_path = os.path.join(subfolder_path, i)  # Save files in the subfolder\n",
    "        dest.append(dest_path)\n",
    "\n",
    "    # Download the files\n",
    "    for file_url, dest_path in zip(files, dest):\n",
    "        print(f\"Downloading {file_url} to {dest_path}\")\n",
    "        response = requests.get(file_url)\n",
    "        with open(dest_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "# List of URLs to process\n",
    "urls = [\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2020-08/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2020-09/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2020-10/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2020-11/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2020-12/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2021-01/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2021-02/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2021-03/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2021-04/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2021-05/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2021-06/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2021-07/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2021-08/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2021-09/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2021-10/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2021-11/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2021-12/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2022-01/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2022-02/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2022-03/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2022-04/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2022-05/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2022-06/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2022-07/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2022-08/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2022-09/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2022-10/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2022-11/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2022-12/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2023-01/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2023-02/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2023-03/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2023-04/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2023-05/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2023-06/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2023-07/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2023-08/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2023-09/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2023-10/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2023-11/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2023-12/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2024-01/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2024-02/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2024-03/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2024-04/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2024-05/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2024-06/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2024-07/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2024-08/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2024-09/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2024-10/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2024-11/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2024-12/\",\n",
    "    \"https://sdge.sdsc.edu/data/sdge/ens_gfs_001/2025-01/\"\n",
    "]\n",
    "\n",
    "# Directory to save the downloaded files\n",
    "save_directory = \"ens_gfs_001\"\n",
    "\n",
    "# Process each URL\n",
    "for url in urls:\n",
    "    download_files_from_url(url, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xarray netCDF4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'ens_gfs_001/2020-08/dfmnfdrs_202008152000Z.nc'\n",
    "\n",
    "dataset = xr.open_dataset(file_path)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dataset.to_dataframe()\n",
    "\n",
    "# Display the DataFrame\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path2 = 'ens_gfs_001/2020-08/wrfout_d02_202008151600Z.nc'\n",
    "\n",
    "dataset2 = xr.open_dataset(file_path2)\n",
    "\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dataset2.to_dataframe()\n",
    "\n",
    "# Display the DataFrame\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique (latitude, longitude) pairs from both dataframes\n",
    "df1_pairs = set(zip(df1['latitude'], df1['longitude']))\n",
    "df2_pairs = set(zip(df2['latitude'], df2['longitude']))\n",
    "\n",
    "# Find the intersection of the two sets\n",
    "common_pairs = df1_pairs.intersection(df2_pairs)\n",
    "\n",
    "# Count the number of unique pairs in the intersection\n",
    "num_common_pairs = len(common_pairs)\n",
    "\n",
    "print(f\"The number of unique (latitude, longitude) pairs in both dataframes: {num_common_pairs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique (latitude, longitude, time) triplets from both dataframes\n",
    "df1_triplets = set(zip(df1['latitude'], df1['longitude'], df1.index))\n",
    "df2_triplets = set(zip(df2['latitude'], df2['longitude'], df2.index))\n",
    "\n",
    "# Find the intersection of the two sets\n",
    "common_triplets = df1_triplets.intersection(df2_triplets)\n",
    "\n",
    "# Count the number of unique triplets in the intersection\n",
    "num_common_triplets = len(common_triplets)\n",
    "\n",
    "print(f\"The number of unique (latitude, longitude, time) triplets in both dataframes: {num_common_triplets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_latitude = 31.855812\n",
    "target_longitude = -121.308640\n",
    "\n",
    "# Filter rows in df1\n",
    "filtered_df1 = df1[(df1['latitude'] == target_latitude) & (df1['longitude'] == target_longitude)]\n",
    "\n",
    "# Filter rows in df2\n",
    "filtered_df2 = df2[(df2['latitude'] == target_latitude) & (df2['longitude'] == target_longitude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df1 = filtered_df1.reset_index() \n",
    "filtered_df2 = filtered_df2.reset_index()\n",
    "\n",
    "combined = pd.merge(filtered_df1, filtered_df2, on=['latitude', 'longitude', 'time'], how='inner', suffixes=('_df1', '_df2'))\n",
    "\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_cleaned = combined.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_related_columns = ['energy_release_component',\n",
    "       'ignition_component', 'fire_intensity_level', 'forward_rate_of_spread',\n",
    "       'spread_component', 'burning_index', 'flame_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(combined[fire_related_columns].values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors and move to GPU if available\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention_weights = nn.Linear(feature_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention_scores = self.attention_weights(x)  # Shape: (batch_size, 1)\n",
    "        attention_scores = torch.softmax(attention_scores, dim=1)\n",
    "\n",
    "        weighted_features = x * attention_scores  # Shape: (batch_size, feature_dim)\n",
    "        return weighted_features, attention_scores\n",
    "\n",
    "class WildfireRiskModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob=0.5):\n",
    "        super(WildfireRiskModel, self).__init__()\n",
    "        self.attention = AttentionLayer(input_size)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size // 2)\n",
    "        self.fc4 = nn.Linear(hidden_size // 2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply attention\n",
    "        weighted_features, attention_scores = self.attention(x)\n",
    "        out = self.fc1(weighted_features)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        return out, attention_scores\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128 \n",
    "output_size = 1  \n",
    "model = WildfireRiskModel(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # L2 regularization\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, _ = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs, _ = model(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# Model evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs, attention_scores = model(X_test)\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "    # Convert predictions and attention scores to numpy so they're usable\n",
    "    predictions = test_outputs.cpu().numpy()\n",
    "    actuals = y_test.cpu().numpy()\n",
    "    attention_scores = attention_scores.cpu().numpy()\n",
    "\n",
    "# Model save\n",
    "torch.save(model.state_dict(), \"wildfire_risk_model_with_attention.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_function(score, other_variables, weights, threshold=100, psps_trigger_value=100):\n",
    "    if score >= threshold:\n",
    "        return psps_trigger_value\n",
    "    \n",
    "    weighted_sum = weights[0] * score  # Weight for the score\n",
    "    for i, var in enumerate(other_variables):\n",
    "        weighted_sum += weights[i + 1] * var  # Weights for other variables\n",
    "    \n",
    "    return weighted_sum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
